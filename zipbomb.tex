\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix2019_v3}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fancyvrb}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{pifont}
\usepackage{threeparttable}
\usepackage{xcolor}
\usepackage[binary-units,number-unit-product=~]{siunitx}

\urlstyle{same}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
% Better look for citations that include a section reference like \cite[\S 3]{foobar}.
\renewcommand{\citemid}{~}

% Disable metadata for reproducible PDF.
% https://tex.stackexchange.com/a/313605
\ifpdf
\pdfinfoomitdate=1
\pdftrailerid{}
\pdfsuppressptexinfo=-1
\hypersetup{pdfcreator={},pdfproducer={}}
\fi

\newcommand{\kB}{\kilo\byte}
\newcommand{\MB}{\mega\byte}
\newcommand{\GB}{\giga\byte}
\newcommand{\TB}{\tera\byte}
\newcommand{\PB}{\peta\byte}
\newcommand{\EB}{\exa\byte}
\newcommand{\EiB}{\exbi\byte}

\newcommand{\CDH}{\mathrm{CDH}}
\newcommand{\LFH}{\mathrm{LFH}}
\newcommand{\Q}{\mathrm{Q}}
\newcommand{\C}{\mathrm{C}}
\newcommand{\OPT}{\mathrm{OPT}}
\newcommand{\bulkdeflate}{\mbox{bulk\_deflate}}
\newcommand{\CRC}{\mbox{CRC-32}}

\newcommand{\yes}{\cellcolor{YellowGreen}\checkmark}
\newcommand{\no}{\cellcolor{Bittersweet}\ding{55}}
\newcommand{\maybe}[1]{\cellcolor{GreenYellow}#1}

\begin{document}

\date{}

\title{\Large \bf A better zip bomb}

\author{
% ANON
% {\rm David Fifield}
}

\maketitle

\begin{abstract}
We show how to construct a
\emph{non-recursive} zip bomb
that achieves a high compression ratio by
overlapping files inside the zip container.
``Non-recursive'' means that it does not rely on
a decompressor's recursively unpacking zip files nested within zip files:
it expands fully after a single round of decompression.
The output size increases quadratically in the input size,
reaching a compression ratio of over 28~million
($\SI{10}{\MB} \rightarrow \SI{281}{\TB}$)
at the limits of the zip format.
Even greater expansion is possible using
64-bit extensions.
The construction uses only the most common compression algorithm, DEFLATE,
and is compatible with most zip implementations.
\end{abstract}


\section{Introduction}
\label{sec:intro}

Compression bombs that use the zip format
must cope with the fact that DEFLATE,
the only compression algorithm universally supported by zip parsers,
cannot achieve a compression ratio greater than
\num{1032}~\cite{zlib_tech}.
For this reason, zip bombs typically rely on recursive decompression,
nesting zip files within zip files to get an extra factor of 1032 with each layer.
But the trick only works on parsers that
unzip recursively, and most do not.
The best-known zip bomb, 42.zip~\cite{42.zip},
expands to a formidable \SI{4.5}{\PB}
if all six of its layers are recursively unzipped,
but a measly \SI{0.6}{\MB} at the top layer.
Zip quines, like those of
Ellingsen~\cite{ellingsen}
and Cox~\cite{cox},
which contain a copy of themselves
and thus expand infinitely if recursively unzipped,
are likewise perfectly safe to unzip once.

This article shows how to construct a non-recursive zip bomb
whose compression ratio surpasses the DEFLATE limit of 1032.
It works by overlapping files inside the zip container,
in order to include a ``kernel'' of highly compressed data
in multiple files, without making multiple copies of it.
The zip bomb's output size grows quadratically in the input size; i.e.,
the compression ratio gets better as the input file gets bigger.
The construction depends on specific features of both zip and DEFLATE---it
is not directly portable to other container formats or compression algorithms.
It is compatible with most zip parsers,
the exceptions being ``streaming'' parsers that
parse in one pass without first consulting the zip file's central directory.
The zip bomb construction tries to balance
two competing goals:
\begin{itemize}
\item
Maximize the compression ratio.
We define the compression ratio as the the sum of the sizes
of all the files contained the in the zip file,
divided by the size of the zip file itself.
It does not count filenames or other filesystem metadata,
only contents.
\item
Be compatible.
Zip is a tricky format and parsers differ, especially
around edge cases and optional features.
Avoid taking advantage of tricks that only work with certain parsers.
We will remark on ways to increase the efficiency of the zip bomb
that come with some loss of compatibility.
\end{itemize}

The construction we will develop is tunable for different output sizes.
For the sake of comparison and discussion,
we will produce three concrete examples.
These examples and others are compared in \autoref{tab:comparison}.

\begin{center}
\begin{tabular}{lr@{~}l@{${} \rightarrow {}$}r@{~}ll}
zbsm.zip & \num{42} & kB & \num{5.5}   & GB & \autoref{sec:allocation} \\
zblg.zip & \num{10} & MB & \num{281.4} & TB & \autoref{sec:allocation} \\
zbxl.zip & \num{46} & MB & \num{4.5}   & PB & \autoref{sec:zip64}
\end{tabular}
\end{center}


\section{Structure of a zip file}
\label{sec:zipstructure}

A~zip file consists of
a \emph{central directory} which references
\emph{files}.
Refer to \autoref{fig:normal}.

The central directory is at the end of the zip file.
It is a list of \emph{central directory headers}.
Each central directory header contains metadata for a single file,
like its filename and \CRC\ checksum,
and a backwards pointer to the file's local file header.
A~central directory header is 46~bytes long,
plus the length of the filename.

A~file consists of a \emph{local file header}
followed by compressed \emph{file data}.
The local file header is 30~bytes long,
plus the length of the filename.
It contains a redundant copy
of the metadata from the central directory header,
and the compressed and uncompressed sizes of the file data
which follows.
Zip is a container format, not a compression algorithm.
Each file's data is compressed by itself,
without reference to any other file,
using an external compression algorithm---usually DEFLATE~\cite{rfc1951}.

This description of the zip format omits many details that
are not needed to understand the construction of the zip bomb.
For full information,
refer to the file format specification~\cite{appnote},
particularly Section~4.3.

\begin{table*}
\centering
\begin{threeparttable}
\setlength{\tabcolsep}{0.45em}
\begin{tabular}{rr|rr@{}l|rr@{}l}
&
&
\multicolumn{3}{c|}{\textbf{non-recursive}} &
\multicolumn{3}{c}{\textbf{recursive}}
\\
&
zipped size &
unzipped size &
\multicolumn{2}{r|}{ratio\phantom{~thousand}} &
unzipped size &
\multicolumn{2}{r}{ratio\phantom{~thousand}}
\\
Cox quine~\cite{cox} &
\num{440} &
\num{440} &
\num{1}&.0 &
$\infty$ &
$\infty$&
\\
Ellingsen quine~\cite{ellingsen} &
\num{28809} &
\num{42569} &
\num{1}&.5 &
$\infty$ &
$\infty$&
\\
42.zip~\cite{42.zip} &
\num{42374}\tnote{*} &
\num{558432} &
\num{13}&.2 &
\num{4507981343026016} &
106&~billion
\\
zbsm.zip (this method) &
\num{42374} &
\num{5461307620} &
129&~thousand &
\num{5461307620} &
129&~thousand
\\
zblg.zip (this method) &
\num{9893525} &
\num{281395456244934} &
28&~million &
\num{281395456244934} &
28&~million
\\
zbxl.zip (this method, Zip64)&
\num{45876952} &
\num{4507981427706459} &
98&~million &
\num{4507981427706459} &
98&~million
% \\
% zbxxl.zip &
% \num{2961656712} &
% \num{18446744085437447493} &
% 6&~billion &
% \num{18446744085437447493} &
% 6&~billion
\end{tabular}
\caption{
Comparison of zip bomb compression ratios.
}
\label{tab:comparison}
\begin{tablenotes}
\item [*]
There are two versions of 42.zip,
an \href{https://web.archive.org/web/20120222083624/http://www.unforgettable.dk/}{older version} of \num{42374} bytes,
and a \href{https://web.archive.org/web/20120301154142/http://www.unforgettable.dk/}{newer version} of \num{42838} bytes.
The only difference between them is that the newer version has a password.
We consider only the older version.
\end{tablenotes}
\end{threeparttable}
\end{table*}

\begin{figure*}
\includegraphics{figures/normal}
\caption{
A normal zip file
(\autoref{sec:zipstructure}).
}
\label{fig:normal}
\end{figure*}


\section{The first insight: overlapping files}
\label{sec:overlap}

By compressing a long string of repeated bytes,
we can produce a \emph{kernel}
of highly compressed data.
By itself, the kernel's compression ratio cannot
exceed the DEFLATE limit of \num{1032},
so we want a way to reuse the kernel in many files,
without having to make a separate copy of it in each file.
We can do it by overlapping files:
making many central directory headers point to
a single file whose data is the kernel.
See \autoref{fig:overlap}.

\begin{figure*}
\includegraphics{figures/overlap}
\caption{
Full-overlap zip bomb construction
(\autoref{sec:overlap}).
This construction has problems with compatibility,
because filenames do not agree between the
central directory headers and the local file headers.
The ``kernel'' is a block of highly compressed data,
reused in every file.
}
\label{fig:overlap}
\end{figure*}

Let's look at an example to see how this construction affects the compression ratio.
Suppose the kernel is \SI{1000}{bytes} and
decompresses to \SI{1}{\MB}.
Then the first \SI{1}{\MB} of output ``costs''
\SI{1078}{bytes} of input:
\SI{31}{bytes} for a local file header (including a 1-byte filename),
\SI{47}{bytes} for a central directory header (including a 1-byte filename), and
\SI{1000}{bytes} for the kernel itself.
But every \SI{1}{\MB} of output
after the first costs only \SI{47}{bytes}---we don't need another local file header or copy of the kernel,
only an additional central directory header.
So while the first copy of the kernel has a compression ratio of
$\num{1000000}/\num{1078}\approx \num{928}$,
each additional copy pulls the ratio closer to
$\num{1000000}/\num{47}\approx \num{21277}$.
A~bigger kernel raises the ceiling.

The problem with this idea is a lack of compatibility.
The file metadata---specifically the filename---doesn't match
between the central directory header and local file header,
and some parsers balk at that.
See \autoref{tab:compatibility}.
Info-ZIP UnZip~\cite{infozip-unzip}
(the standard Unix \texttt{unzip} program)
extracts the files, but with warnings:

{\small
\begin{Verbatim}[commandchars=\\\{\}]
$ \textbf{unzip overlap.zip}
  inflating: A
B:  mismatching "local" filename (A),
         continuing with "central" filename version
  inflating: B
\textit{...}
\end{Verbatim}
}

\noindent
And the Python zipfile module~\cite{python-zipfile}
\href{https://github.com/python/cpython/blob/v3.7.0/Lib/zipfile.py#L1486-L1489}{throws an exception}
when filenames don't match:

{\small
\begin{Verbatim}[commandchars=\\\{\}]
$ \textbf{python3 -m zipfile -e overlap.zip .}
Traceback (most recent call last):
\textit{...}
__main__.BadZipFile: File name in directory 'B' \textbackslash
and header b'A' differ.
\end{Verbatim}
}

Next we will see how to modify this construction
for filename consistency,
while still retaining most of the advantage
of overlapping files.

% If you are targeting a specific zip parser that you know
% does not check for filename consistency,
% this construction is a good one.
% It grows faster (as a function of the input size)
% than the construction of the next section,
% which has better compatibility
% because it uses separate local file headers.

% Somewhat related, Plötz et~al.~\cite[\S 4]{SAR-PR-2006-04}
% used overlapping files to achieve
% almost complete replication of the zip file itself.


\section{The second insight:\\quoting local file headers}
\label{sec:quote}

We need to somehow separate the local file headers for each file,
while still reusing a single kernel.
We'll use a feature of DEFLATE, non-compressed blocks,
to ``quote'' local file headers that occur later in the file
so that they appear to be part of the same DEFLATE stream
that terminates in the kernel.
Every local file header,
except the first,
will be interpreted two ways:
as code (part of the structure of the zip file format)
and as data (part of the output file data).

A~DEFLATE stream is a sequence of
blocks~\cite[\S 3.2.3]{rfc1951},
where each block can be compressed or non-compressed.
Compressed blocks are what we usually think of;
for example the kernel is one big compressed block.
But there are also non-compressed blocks,
which start with a
5-byte header~\cite[\S 3.2.4]{rfc1951}
that means simply, ``output the next $n$ bytes verbatim.''
Decompressing a non-compressed block means only stripping the 5-byte header.
Compressed and non-compressed blocks may be intermixed freely
in a DEFLATE stream.
The output is the concatenation of
decompressing all the blocks in order.

It is easiest to understand the construction from the inside out,
starting with the last file and working backwards to the first.
Refer to \autoref{fig:quote}.
Start by inserting the kernel, which will form the end of file data for every file.
Prepend a local file header $\LFH_N$
and add a central directory header $\CDH_N$ that points to it.
Set the ``compressed size'' metadata field in $\LFH_N$ and $\CDH_N$ to the compressed size of the kernel.
Now, insert before $\LFH_N$ a 5-byte non-compressed block header (colored green in the diagram)
whose length field is equal to the size of $\LFH_N$.
For the second time,
prepend a local file header $\LFH_{N-1}$
and add a central directory header $\CDH_{N-1}$ that points to it.
Set the ``compressed size'' metadata field in both headers to the compressed size of the kernel,
\emph{plus} the size of the non-compressed block header (\SI{5}{bytes}),
\emph{plus} the size of $\LFH_N$.

\begin{figure*}
\includegraphics{figures/quote}
\caption{
Quoted-overlap zip file construction
(\autoref{sec:quote}).
Each file contains the local file headers of all the files which follow it,
as well as the kernel.
The green parts stand for DEFLATE non-compressed blocks.
}
\label{fig:quote}
\end{figure*}

At this point the zip file contains two files.
Let's walk through what a zip parser would see while parsing it.
Supposed the compressed size of the kernel is \SI{1000}{bytes}
and the size of $\LFH_N$ is \SI{31}{bytes}.
We start at $\CDH_{N-1}$
and follow the pointer to $\LFH_{N-1}$.
The first file's filename is ``Y'' and
the compressed size of its file data is \SI{1036}{bytes}.
Interpreting the following \SI{1036}{bytes} as a DEFLATE stream,
we first encounter the 5-byte header of a non-compressed block
that says to copy the next \SI{31}{bytes}.
We write the next \SI{31}{bytes},
which happen to be $\LFH_N$,
as output to the file ``Y''.
Moving on in the DEFLATE stream, we find a compressed block (the kernel),
which we decompress to file ``Y''.
Now we have reached the end of the compressed data and are done with file ``Y''.
Proceeding to the next file, we follow the pointer from $\CDH_N$
to $\LFH_N$ and find a file with filename ``Z''
and compressed size \SI{1000}{bytes}.
Interpreting those \SI{1000}{bytes} as a DEFLATE stream,
we immediately encounter a compressed block (the kernel again)
and decompress it to the file ``Z''.
Now we have reached the end of the final file and are done.
The output file ``Z'' contains the decompressed kernel;
the output file ``Y'' is be the same, except additionally prefixed by
$\LFH_N$, \SI{31}{bytes} copied literally from the zip file.

The zip file is constructed by repeating the quoting procedure.
Each new file adds a central directory header,
a local file header,
and a non-compressed block to quote the next local file header.
Compressed file data is generally a chain of DEFLATE non-compressed blocks
(the quoted local file headers)
followed by the compressed kernel.
Each byte in the kernel contributes about
$\num{1032}N$ to the output size,
because it is part of all $N$ files.
The output files are not all the same size:
those that appear earlier in the zip file
are larger than those that appear later,
because they contain more local file headers.
The contents of the output files are nonsense,
but no one said they had to make sense.

This quoted-overlap construction has better compatibility
than the full-overlap construction of \autoref{sec:overlap},
but the compatibility comes at the expense of the compression ratio.
There, each added file cost only a central directory header;
but here, it costs a central directory header,
a local file header,
and another \SI{5}{bytes} for the quoting header.


\section{Optimization}
\label{sec:optimization}

Now that we have the basic zip bomb construction,
we will try to make it as efficient as possible.
We want to answer two questions:

\begin{itemize}
\item For a given zip file size, what's the maximum compression ratio?
\item What's the maximum compression ratio, given the limits of the zip format?
\end{itemize}

\subsection{Kernel compression}
\label{sec:bulkdeflate}

It is worthwhile to compress the kernel as densely as possible,
because every decompressed byte gets magnified by a factor of $N$.
To that end, we use a custom DEFLATE compressor
called \bulkdeflate,
specialized for compressing
a string of repeated bytes.

%          engine compressed_size max_uncompressed_size
% 1: bulk_deflate           21090              21749401
% 2:         zlib           21090              21723602
% 3:       zopfli           21090              21734018

All decent DEFLATE compressors will approach a compression ratio of \num{1032}
when given an infinite stream of repeating bytes,
but we care more about specific finite sizes
than asymptotics.
\autoref{fig:max-uncompressed-size} shows the
maximum amount of output
that can result from uncompressing a DEFLATE stream of a given size,
for \bulkdeflate\ and other implementations.
\bulkdeflate\ compresses more data
into the same space than the general-purpose compressors:
about \SI{26}{\kB} more than zlib and Info-ZIP,
and about \SI{15}{\kB} more than Zopfli~\cite{zopfli},
a compressor that trades speed for density.

\begin{figure}
\includegraphics{data/max_uncompressed_size}
\caption{
Comparison of DEFLATE compressors
on a string of repeated bytes.
}
\label{fig:max-uncompressed-size}
\end{figure}

The price of \bulkdeflate's high compression ratio is a lack of generality.
\bulkdeflate\ can only compress strings of a single repeated byte,
and only those of specific lengths,
namely $517 + 258 k$ for integer $k \ge 0$.
Besides compressing densely, \bulkdeflate\ is fast,
doing essentially constant work regardless of the input size
(aside from the $O(n)$ work of actually writing out the compressed string).

\subsection{Filenames}
\label{sec:filenames}

For our purposes, filenames are mostly dead weight.
While filenames do contribute something to the output size
by virtue of being part of quoted local file headers,
a byte in a filename does not contribute nearly as much to the output size
as does a byte in the kernel.
So we want filenames to be as short as possible,
while keeping them all distinct,
and subject to compatibility considerations.

The first compatibility consideration is filename encoding.
The zip format specification states that filenames
are to be interpreted as CP~437,
or \mbox{UTF-8} if a certain flag bit is set~\cite[Appendix~D]{appnote}.
But this is a major point of incompatibility
between zip parsers,
which may interpret filenames as being in
some fixed or locale-specific encoding, or ignore the \mbox{UTF-8} bit, for example.
So for compatibility, we must limit ourselves to characters
that have the same encoding in both
CP~437 and \mbox{UTF-8},
namely, the \num{95} printable characters of \mbox{US-ASCII}.
% https://bugs.python.org/issue10614
% https://bugs.python.org/issue10972
% https://github.com/thejoshwolfe/yauzl/issues/84

We are further restricted by filesystem naming limitations.
Some filesystems are case-insensitive, so ``a'' and ``A'' do not count as distinct names.
Common filesystems like FAT32 prohibit certain characters like
`*' and `?'~\cite[\S Limits]{wiki-fs}.

As a safe compromise between all these considerations,
our zip bomb will use filenames consisting of characters
drawn from a 36-character alphabet
that does not
rely on case distinctions
or use special characters:

\begin{center}
\setlength{\tabcolsep}{2pt}
\begin{tabular}{cccccccccccccccccc}
0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & A & B & C & D & E & F & G & H \\
I & J & K & L & M & N & O & P & Q & R & S & T & U & V & W & X & Y & Z
\end{tabular}
\end{center}

\noindent
Filenames are generated in the obvious way,
cycling each position through the possible characters
and adding a position on overflow:

\begin{center}
\setlength{\tabcolsep}{2pt}
\begin{tabular}{rrrrrl}
``\mbox{0}'', &
``\mbox{1}'', &
``\mbox{2}'', &
\ldots, &
``\mbox{Z}'',
\\
``\mbox{00}'', &
``\mbox{01}'', &
``\mbox{02}'', &
\ldots, &
``\mbox{0Z}'',
\\
\ldots,
\\
``\mbox{Z0}'', &
``\mbox{Z1}'', &
``\mbox{Z2}'', &
\ldots, &
``\mbox{ZZ}'',
\\
``\mbox{000}'', &
``\mbox{001}'', &
``\mbox{002}'', &
\ldots
\end{tabular}
\end{center}

\noindent
There are $36$ filenames of length~\num{1},
$36^2$ filenames of length~\num{2}, and so on.
Using this scheme, four character positions are enough for
\num{1727604} filenames.

% The length of the $n$\/th filename is
% $\lfloor \log_{36}((n + 1) / \frac{36}{35})\rfloor + 1$.
% The sum of the lengths of the first $n$ filenames is
% $dn - ((36^d - 1) \frac{36}{35^2} - \frac{d}{35})$,
% where $d = \lfloor \log_{36}(n / \frac{36}{35})\rfloor$.
% https://oeis.org/A014824

Given that the $N$ filenames in the zip file
are generally not all of the same length,
how should we order them,
shortest to longest or longest to shortest?
A~little reflection shows that it is better to
put the longest names last, because those names are the most quoted.
Ordering filenames longest last
adds over \SI{900}{\MB} of output
to the \mbox{zblg.zip} zip bomb we will see in \autoref{sec:allocation},
compared to ordering them longest first.
It is only a minor optimization, though,
as those \SI{900}{\MB} comprise only \num{0.0003}\%
of the total output size.

% $ unzip -l zblg.zip | tail -n 1
% 281395456244934                     65534 files
% $ unzip -l zblg.rev.zip | tail -n 1
% 281394526372494                     65534 files
% $ python
% >>> 281395456244934 - 281394526372494
% 929872440
% >>> 929872440 / 281395456244934. * 100
% 0.00033045041039703735

\subsection{Kernel size}
\label{sec:allocation}

The quoted-overlap construction
allows us to place a compressed kernel of data,
and then cheaply copy it many times.
For a given zip file size~$X$,
how much space should we devote to storing the kernel,
and how much to making copies?

To find the optimum balance,
we only have to optimize the single variable $N$,
the number of files in the zip file.
Every value of $N$ requires
a certain amount of overhead for
central directory headers,
local file headers,
quoting block headers, and filenames.
All the remaining space can be taken up by the kernel.
Because $N$ has to be an integer,
and there are only so many files you can fit
before the kernel size drops to zero,
it suffices to test every possible value of $N$
and select the one that yields the most output.

Applying the optimization procedure to $X = \num{42374}$,
the size of 42.zip,
finds a maximum at $N = 250$.
Those \num{250} files require \SI{21195}{bytes} of overhead,
leaving \SI{21179}{bytes} bytes for the kernel.
The kernel, by itself, decompresses to \SI{21841249}{bytes}
(a~ratio of \num{1031.3}).
The \num{250} copies of the decompressed kernel,
plus the little bit extra that comes from the quoted local file headers,
produces an overall unzipped output of
\SI{5461307620}{bytes}
and a~compression ratio of 128~thousand.
This zip bomb is \mbox{zbsm.zip}---refer to \autoref{tab:comparison}.

Optimization produced an almost even split
of the available space
between the kernel and the file headers.
It is not a coincidence.
Let's look at a simplified model of the quoted-overlap construction.
In the simplified model,
we ignore filenames,
as well as the slight increase in output file size
due to quoting local file headers.
Analysis of the simplified model will show that the optimum
split between kernel and file headers is approximately even,
and that the output size grows quadratically in the input size
when space is allocated optimally.

Define some constants and variables:

\begin{align*}
X & & & \mbox{zip file size (take as fixed)} \\
N & & & \mbox{number of files (variable to optimize)} \\
\CDH &= \num{46} & & \mbox{size of a central directory header} \\
\LFH &= \num{30} & & \mbox{size of a local file header} \\
\Q   &=  \num{5} & & \mbox{size of a quoting block header} \\
\C   &\approx \num{1032} & & \mbox{compression ratio of the kernel}
\end{align*}

% JACAL session to do the algebra and calculus:
% 
% # S_X(N)
% e1 : C * N * (X - (N * (LFH + CDH) + (N - 1) * Q)));
%                                                   ^
% 
%                        2             2
% e1: (- C CDH - C LFH) N  + (C N - C N ) Q + C N X
% 
% # S_X(N) as a polynomial in N
% e2 : coeffs(e1, N);
% 
% e2: [0, C Q + C X, - C CDH - C LFH - C Q]
% 
% # S'_X(N)
% e3 : diff(e1, N);
% 
% e3: (- 2 C CDH - 2 C LFH) N + (C - 2 C N) Q + C X
% 
% # S'_X(N) as a polynomial in N
% e4 : coeffs(e3, N);
% 
% e4: [C Q + C X, - 2 C CDH - 2 C LFH - 2 C Q]
% 
% # Solve S'_X(N) = 0. e5 == N_OPT
% e5 : suchthat(N, e3);
% 
%            Q + X
% e5: - - - - - - - - - -
%     2 CDH + 2 LFH + 2 Q
% 
% # H(N_OPT)
% e6 : e5 * (LFH + CDH) + (e5 - 1) * Q;
% 
%     - Q + X
% e6: - - - -
%        2
% 
% # S_X(N_OPT)
% e7 : -C * (CDH + LFH + Q) * e5^2 + C * (Q + X) * e5;
% 
%        2                2
%     C Q  + 2 C Q X + C X
% e7: - - - - - - - - - - -
%      4 CDH + 4 LFH + 4 Q

Let $H(N)$
be the amount of header overhead required for $N$ files.
Refer to \autoref{fig:quote}
to understand where this formula comes from.

\begin{align*}
H(N) &= N\cdot(\CDH + \LFH) + (N - 1)\cdot\Q
\end{align*}

The space remaining for the kernel is
$X - H(N)$.
The total unzipped size
$S_X(N)$
is the size of $N$ copies
of the kernel,
decompressed at ratio~$\C$.
(In this simplified model we ignore
the minor additional expansion from quoted local file headers.)

\begin{align*}
S_X(N) &= (X - H(N)) \, \C \, N \\
       &= (X - (N \cdot (\CDH + \LFH) + (N - 1)\cdot\Q)) \, \C \, N \\
       &= -(\CDH + \LFH + \Q) \, \C \, N^2 + (X + \Q) \, \C \, N
\end{align*}

$S_X(N)$
is a polynomial in $N$,
so its maximum must be at a place where the derivative
$S'_X(N)$
is zero.
Taking the derivative and finding the zero gives us
$N_\OPT$,
the optimal number of files.

\begin{align*}
S'_X(N_\OPT) &= - 2 (\CDH + \LFH + \Q) \, C \, N_\OPT + (X + \Q) \, \C \\
           0 &= - 2 (\CDH + \LFH + \Q) \, C \, N_\OPT + (X + \Q) \, \C \\
     N_\OPT  &= \frac{X + \Q}{2 (\CDH + \LFH + \Q)}
\end{align*}

$H(N_\OPT)$
gives the optimal amount of space to allocate for file headers.
It is independent of $\CDH$, $\LFH$, and $\C$,
and close to $X/2$.

\begin{align*}
H(N_\OPT) &= N_\OPT\cdot(\CDH + \LFH) + (N_\OPT - 1)\cdot\Q \\
          &= \frac{X - \Q}{2}
\end{align*}

$S_X(N_\OPT)$
is the total unzipped size
when the allocation is optimal.
From this we see that the output size grows quadratically
in the input size.

\begin{align}
\label{eq:opt}
S_X(N_\OPT) &= \frac{(X + \Q)^2 \, \C}{4(\CDH + \LFH + \Q)}
\end{align}

\begin{quote}
\sl
Note to reviewers:
we would appreciate advice
on adapting the simplified model
so that $S_X(N_\OPT)$ is
a true lower bound on the zip bomb's growth.
The problem with the analysis above is that while it slightly underestimates
the output size by ignoring the contribution of quoted local file headers,
it also overestimates the size of the kernel
by giving the kernel the bytes that would be required to store filenames.
To be precise, $H(N)$ needs an additional term
that upper-bounds the space required for $N$ filenames.
The bounds we tried led to messy and unenlightening formulas.
If it helps, the length of the $n$\/th filename is
$\lfloor \log_{36}((n + 1) / \frac{36}{35})\rfloor + 1 =
\lfloor \log_{36}(n + 1) + \log_{36}(35)\rfloor = O(\log n)$.
If we let $d$ be the length of the longest filename,
the sum of the lengths of the first $n$ filenames is
$d\cdot(n+1) - ((36^d - 1) \frac{36}{35^2} - \frac{d}{35}) = O(n\log n)$.
This comes from the observation that all filenames have length $\ge 1$,
all but $36$ have length $\ge 2$,
all but $36+36^2$ have length $\ge 3$, and in general
all but $36+36^2+\cdots+36^{i-1} = \frac{36^i-1}{35}-1$ have length $\ge i$,
for $1 \le i \le d$.
Therefore the sum of lengths is \\
$\phantom{=}\sum_{i=1}^d n - (\frac{36^i-1}{35}-1) \\
= d\cdot(n+1) - \sum_{i=1}^d \frac{36^i-1}{35} \\
= d\cdot(n+1) - ((36^d - 1)\frac{36}{35^2} - \frac{d}{35})$,\\
where the last equality comes from adapting a formula found in
\url{https://oeis.org/A014824}.
See the comment on \texttt{sum\_filename\_lengths}
in \mbox{optimize.R} in the source code.
\end{quote}

As we make the zip file larger and larger,
eventually we run into the limits of the zip format.
A~zip file can contain at most $2^{16}-1$ files
and each file can have an uncompressed size of at most $2^{32}-1$ bytes.
Worse than that:
some implementations (see \autoref{tab:compatibility})
take the maximum possible values
as an indicator of the presence of 64-bit extensions (\autoref{sec:zip64}),
so our limits are actually $2^{16}-2$ and $2^{32}-2$.
% https://github.com/golang/go/commit/4aedbf5be4631693f774063410707ef467ca78e7
% https://github.com/golang/go/commit/b6c5edae7c0e9dd6d12dbb8f1c9638dea45f9464
It happens that the first limit we hit is the one on uncompressed file size.
At a zip file size of \SI{8319377}{bytes},
naive optimization would give us a file count of \num{47837}
and a largest file with an impossible uncompressed size of
$2^{32}+311$ bytes.
% $compressed_size
% [1] 4160277
% 
% $num_files
% [1] 47837
% 
% [1] "zipped size" "8319377"
% [1] "unzipped size"   "205420672417247"
% [1] 4294967607

Accepting that we cannot increase $N$ nor the size of the kernel without bound,
we would like find the maximum compression ratio achievable
while remaining within the limits of the zip format.
The way to proceed is to make the kernel as large as possible,
and have the maximum number of files.
Even though we can no longer maintain an even split
between the kernel and file headers,
each added file \emph{does} increase the compression ratio---just
not as fast as it would if we were able to keep growing the kernel at the same time.
In fact, as we add files we will need to \emph{decrease} the size of the kernel
to make room for the maximum file size
that gets slightly larger with each added file.

The plan results in \mbox{zblg.zip}, a zip file
that contains $2^{16}-2$ files and a kernel that decompresses
to $2^{32}-\num{2178825}$ bytes.
Refer to \autoref{tab:comparison}.
The largest file (the first file) uncompresses to
$2^{32} - 56$ bytes.
That is as close as we can get using \bulkdeflate---encoding
the final \SI{54}{bytes} would require more bytes
than they are worth.
The output size of this zip bomb, \SI{281395456244934}{bytes},
is 99.97\% of the theoretical maximum
$(2^{32}-1)\cdot(2^{16}-1)$.
% 65 535 × 0xffffffff = <data value="281470681677825">281 470 681 677 825</data>.
Any major improvements to the compression ratio can only come
from reducing the input size,
not increasing the output size.
% >>> (2**32-1)*65535 - 281399752637796
% 70929040029
% >>> 70929040029. / ((2**32-1)*65535)
% 0.00025199441592352524
% >>> 1 - _
% 0.9997480055840765
% >>> _ * 100
% 99.97480055840765


\section{Efficient \CRC\ computation}
\label{sec:crc32}

Among the metadata in the central directory header and local file header
is a \CRC\ checksum of the uncompressed file data.
This poses a problem, because calculating the \CRC\ of each file directly
requires doing work proportional to the total \emph{unzipped} size,
which is large by design (it's a zip bomb after all).
We would prefer to do work that in the worst case is
proportional to the \emph{zipped} size.
Two factors work in our advantage:
all files share a common suffix (the kernel),
and the uncompressed kernel is a string of repeated bytes.
We will represent \CRC\ as a matrix product---this
will allow us not only to compute the checksum of the kernel quickly,
but also re-use the computation in every file.
The technique described in this section is a slight extension of the
\texttt{crc32\_combine}
function in zlib,
which Mark Adler has explained~\cite{crc32combine}.

\begin{figure*}
\begin{align*}
M_0 &=
\setcounter{MaxMatrixCols}{33}
\begin{smallmatrix}
0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1
\end{smallmatrix}
&
M_1 &=
\setcounter{MaxMatrixCols}{33}
\begin{smallmatrix}
0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1 \\
0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1 \\
1&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1 \\
0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&1 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&1 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&1 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&1 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&1 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&1 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&1 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&1 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&1 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1
\end{smallmatrix}
\end{align*}
\caption{
The $33\times33$ transformation matrices $M_0$ and $M_1$ that compute
the \CRC\ state change effected by a $0$~bit and a $1$~bit.
Column vectors are stored with the most significant bit at the bottom:
reading the first column from bottom to top, you see
the \CRC\ polynomial constant $\mathrm{edb88320}_{16} = 11101101101110001000001100100000_2$.
The two matrices differ only in the final column, which represents the amount of translation
(except for the bottom $1$ bit, which is part of the homogeneous coordinates representation).
In $M_0$ the final column is zero and
in $M_1$ it is $\mathrm{edb88320}_{16}$, the \CRC\ polynomial constant.
The 1's just above the diagonal represent the
shift operation \texttt{state~>{}>~1}.
}
\label{fig:crc32matrix}
\end{figure*}

You can model \CRC\ as a state machine that updates a 32-bit state register
for each incoming bit.
The basic update operations for a $0$~bit and a $1$~bit are:
{
\small
\begin{verbatim}
uint32 crc32_update_0(uint32 state) {
    // Shift out the least significant bit.
    bit b = state & 1;
    state = state >> 1;
    // If the shifted-out bit was 1, XOR
    // with the CRC-32 constant.
    if (b == 1)
        state = state ^ 0xedb88320;
    return state;
}
uint32 crc32_update_1(uint32 state) {
    // Do as for a 0 bit, then XOR
    // with the CRC-32 constant.
    return crc32_update_0(state) ^ 0xedb88320;
}
\end{verbatim}
}

If you think of the state register as a 32-element binary vector,
and use XOR for addition and AND for multiplication, then
\texttt{crc32\_update\_0} is a linear transformation;
i.e., it can be represented as multiplication by a
$32\times32$ binary transformation matrix.
To see why, observe multiplying a matrix by a vector
is just summing the columns of the matrix,
after multiplying each column by the corresponding element of the vector.
The shift operation \texttt{state~>{}>~1}
is just taking each bit~$i$ of the state vector
and multiplying it by a vector that is $0$ everywhere except at bit $i-1$
(numbering the bits from right to left).
The conditional final XOR \texttt{state~\^~0xedb88320}
that only happens when bit~\texttt{b} is $1$
can instead be represented as first multiplying
\texttt{0xedb88320} by \texttt{b}
and then XORing it into the state unconditionally.

Furthermore, \texttt{crc32\_update\_1} is just
\texttt{crc32\_update\_0} plus (XOR) a~constant.
That makes \texttt{crc32\_update\_1} an
affine transformation:
a~matrix multiplication followed by a translation (addition).
We can represent both the matrix multiplication and the translation
in a single step
if we enlarge the transformation matrix's dimensions to $33\times33$
and append an extra element to the state vector that is always $1$.
(This representation is called
homogeneous coordinates.)

Both operations \texttt{crc32\_update\_0} and \texttt{crc32\_update\_1}
can be represented by a $33\times 33$ transformation matrix.
The matrices $M_0$ and $M_1$ are shown in \autoref{fig:crc32matrix}.
The benefit of representing \CRC\ this way is that matrices compose.
Suppose we want to represent the static change caused by processing
the ASCII character `a', whose binary representation is
$01100001_2$.
We can represent the cumulative \CRC\ state change of those 8~bits
in a single transformation matrix:

\begin{align*}
M_\mathrm{a} &= M_0 \, M_1 \, M_1 \, M_0 \, M_0 \, M_0 \, M_0 \, M_1
\end{align*}

\noindent
And we can represent the state change of a string of repeated `a's
by multiplying many copies of $M_\mathrm{a}$ together---matrix exponentiation.
There is an efficient algorithm to do matrix exponentiation
called the square-and-multiply algorithm,
which allows us to compute $M^n$
in only about $\log_2 n$ steps.
For example, the matrix representing state change
of string of 9~`a's is:

\begin{align*}
(M_\mathrm{a})^9 &= M_\mathrm{a} \, M_\mathrm{a} \, M_\mathrm{a} \, M_\mathrm{a} \, M_\mathrm{a} \, M_\mathrm{a} \, M_\mathrm{a} \, M_\mathrm{a} \, M_\mathrm{a} \\
&= (M_\mathrm{a} \, M_\mathrm{a} \, M_\mathrm{a} \, M_\mathrm{a})^2 \, M_\mathrm{a} \\
&= ((M_\mathrm{a} \, M_\mathrm{a})^2)^2 \, M_\mathrm{a} \\
&= (((M_\mathrm{a})^2)^2)^2 \, M_\mathrm{a}
\end{align*}

The square-and-multiply method is of course useful
to compute $M_\mathrm{kernel}$, the matrix for the uncompressed kernel,
which is a string of repeated bytes.
To produce a \CRC\ checksum value from a matrix,
multiply the matrix by the zero vector.
(The zero vector in homogeneous coordinates, that is:
32~$0$'s followed by a $1$.
Here we omit the minor complication of pre- and post-conditioning the checksum.)
To compute the checksum for every file, we work backwards.
Start by initializing $M := M_\mathrm{kernel}$.
The checksum of the kernel is also the checksum
of the final file~$N$,
so multiply $M$ by the zero matrix and store the resulting checksum in
$\CDH_N$ and $\LFH_N$.
The file data of file~$N-1$ is the same as the file data of file~$N$,
but with an added prefix of $\LFH_N$.
So compute $M_{\LFH_N}$, the state change matrix for $\LFH_N$,
and update $M := M \, M_{\LFH_N}$.
Now $M$ represents the cumulative state change from processing
$\LFH_N$ followed by the kernel.
Continue the procedure, accumulating state change matrices into $M$,
until all the files have been processed.


\section{Extension: Zip64}
\label{sec:zip64}

In \autoref{sec:allocation} we hit a wall on expansion
due to limits of the zip format---it was simply impossible
to produce more than about \SI{281}{\TB} of output.
It is possible to surpass those limits
using Zip64, an extension to the zip format that increases
the size of relevant header fields to 64~bits.
Support for Zip64 is by no means universal,
but it is one of the more commonly implemented extensions---see \autoref{tab:compatibility}.
Zip64 works by appending extra data to file headers.
For our purposes, the effect is to
increasing the size of a central directory header from
\SI{46}{bytes} to \SI{58}{bytes},
and the size of a local directory header from
\SI{30}{bytes} to \SI{50}{bytes}.
Referring to \autoref{eq:opt},
we see that a Zip64-format zip bomb construction
still grows quadratically,
just slightly more slowly because of the larger denominator---this
is visible in \autoref{fig:zipped-size} in the Zip64 line's
lower vertical placement.
In exchange for a loss of compatibility
and slightly slower growth,
we get a practical removal of all file size limits.

\begin{figure}
\includegraphics{data/zipped_size}
\caption{
Zipped size versus unzipped size for various zip bomb constructions.
Note the log–log scales.
The red and orange lines are naive zip bombs without overlapping files.
They have a linear rate of growth,
as evidenced by their 1:1 slope.
The vertical offset of the bzip2 line shows that its compression ratio
is about a thousand times greater than that of DEFLATE.
The blue lines are quoted-overlap zip bombs that use DEFLATE.
(It is not possible to use the quoted-overlap construction with bzip2.)
They have a quadratic rate of growth,
as shown by their 2:1 slope.
Zip64 has a slightly lower compression ratio
for a zip file of the same size,
but it permits output in excess of \SI{281}{\TB}.
Labeled dots indicate specific zip bombs that
also appear in \autoref{tab:comparison}.
}
\label{fig:zipped-size}
\end{figure}

Suppose we want a zip bomb that expands to \SI{4.5}{\PB},
the same size that 42.zip expands to.
Zip64 allows us to reach that high.
Using binary search, we find that the smallest
zip file whose unzipped size exceeds the unzipped size of 42.zip
has a zipped size of \SI{46}{\MB}.
See the \mbox{zbxl.zip} row in \autoref{tab:comparison}.

With Zip64, it's no longer practically interesting to
consider the maximum compression ratio,
because we can just keep increasing the zip file size,
and the compression ratio along with it,
until even the compressed zip file is prohibitively large.
An interesting threshold, though,
is $2^{64}$ bytes
(\SI{18}{\EB} or \SI{16}{\EiB})---that
much data will not fit on most filesystems~\cite[\S Limits]{wiki-fs}.
Binary search finds the smallest zip bomb that produces at least that much output:
it contains 12~million files and has a compressed kernel of \SI{1.5}{\GB}.
The total size of the zip file is \SI{2.9}{\GB} and it unzips
to $2^{64}+\num{11727895877}$ bytes,
having a compression ratio of over 6.2~billion.


\section{Extension: bzip2}
\label{sec:bzip2}

DEFLATE is the most common compression algorithm
used in the zip format, but it is only one of many options~\cite[\S 4.4.5]{appnote}.
bzip2~\cite{bzip2}, while not nearly as compatible as DEFLATE (see \autoref{tab:compatibility}),
is probably the second most commonly supported compression algorithm.
Empirically, bzip2 has a maximum compression ratio of about 1.4~million,
which allows for denser packing of the kernel.
Ignoring the loss of compatibility,
does bzip2 enable a more efficient zip bomb?

% bzip2 starts with a run-length encoding step
% that reduces the length of a string of repeated bytes
% by a factor of~51.
% Then the data is separated into
% <data value="900000">900~<abbr title=kilobyte>kB</abbr></data> blocks
% and each block compressed individually.
% Empirically, one block after run-length encoding
% can compress down to 32~bytes.
% 900 000 × 51 / 32 = 1 434 375.

Yes---but only for small files.
The problem is that bzip2 does not have anything like the
non-compressed blocks of DEFLATE
that we used in \autoref{sec:quote} to quote local file headers.
So it is not possible to reuse the kernel---each file must have
its own copy of it, and therefore the overall compression ratio
is not better than the ratio of any single file.
In \autoref{fig:zipped-size} we see that
bzip2 outperforms DEFLATE-with-quoting
only for zip bombs under about a megabyte.

If you happen to know that a certain zip parser both supports bzip2
and tolerates mismatched filenames,
then you can use the full-overlap construction of \autoref{sec:overlap}.
There is no need for quoting in that construction,
so it is compatible with bzip2.


\begin{table*}
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{rccccccc}
&
\thead{Info-ZIP\\UnZip~6.0~\cite{infozip-unzip}} &
\thead{Python~3.7\\zipfile~\cite{python-zipfile}} &
\thead{Go~1.12\\archive/zip~\cite{golang-archivezip}} &
\thead{yauzl~2.10.0~\cite{yauzl}\\(Node.js)} &
\thead{Nail~\cite{186219}\\examples/zip~\cite{nail-zip}} &
\thead{Android~9.0.0~r1\\libziparchive~\cite{android-libziparchive}} &
\thead{sunzip~0.4~\cite{sunzip}\\(streaming)}
\\
\thead[r]{DEFLATE} &
\yes &
\href{https://github.com/python/cpython/blob/v3.7.0/Lib/zipfile.py#L57}{\yes} &
\href{https://github.com/golang/go/blob/go1.12/src/archive/zip/struct.go#L31}{\yes} &
\href{https://github.com/thejoshwolfe/yauzl/blob/2.10.0/index.js#L520-L521}{\yes} &
\href{https://github.com/jbangert/nail/blob/4bd9cc29c4092abe7a77f8294aff2337bba02ec5/examples/zip/zip.c#L63}{\yes} &
\href{https://android.googlesource.com/platform/system/core/+/refs/tags/android-9.0.0_r1/libziparchive/zip_archive.cc#1059}{\yes} &
\href{https://github.com/madler/sunzip/blob/v0.4/sunzip.c#L1256}{\yes}
\\
\thead[r]{Zip64} &
\href{http://infozip.sourceforge.net/UnZip.html#Release}{\yes} &
\href{https://github.com/python/cpython/blob/v3.7.0/Lib/zipfile.py#L186}{\yes} &
\href{https://github.com/golang/go/blob/go1.12/src/archive/zip/reader.go#L519}{\yes} &
\href{https://github.com/thejoshwolfe/yauzl/tree/2.10.0#limitted-zip64-support}{\yes} &
\href{https://github.com/jbangert/nail/blob/4bd9cc29c4092abe7a77f8294aff2337bba02ec5/examples/zip/zip.c#L103-L125}{\no} &
\href{https://android.googlesource.com/platform/system/core/+/refs/tags/android-9.0.0_r1/libziparchive/zip_archive.cc#168}{\no} &
\href{https://github.com/madler/sunzip/blob/v0.4/sunzip.c#L922}{\yes}
\\
\thead[r]{bzip2} &
\href{http://infozip.sourceforge.net/UnZip.html#Release}{\yes} &
\href{https://github.com/python/cpython/blob/v3.7.0/Lib/zipfile.py#L58}{\yes} &
\href{https://github.com/golang/go/blob/go1.12/src/archive/zip/struct.go#L28-L32}{\no} &
\href{https://github.com/thejoshwolfe/yauzl/blob/2.10.0/index.js#L517-L525}{\no} &
\href{https://github.com/jbangert/nail/blob/4bd9cc29c4092abe7a77f8294aff2337bba02ec5/examples/zip/zip.c#L86}{\no} &
\href{https://android.googlesource.com/platform/system/core/+/refs/tags/android-9.0.0_r1/libziparchive/zip_archive.cc#1061}{\no} &
\href{https://github.com/madler/sunzip/blob/v0.4/sunzip.c#L1256}{\yes}
\\
\thead[r]{tolerates mismatched filenames} &
\maybe{warns} &
\href{https://github.com/python/cpython/blob/v3.7.0/Lib/zipfile.py#L1486-L1489}{\no} &
\href{https://github.com/golang/go/blob/go1.12/src/archive/zip/reader.go#L244}{\yes} &
\href{https://github.com/thejoshwolfe/yauzl/tree/2.10.0#local-file-headers-are-ignored}{\yes} &
\href{https://github.com/jbangert/nail/blob/4bd9cc29c4092abe7a77f8294aff2337bba02ec5/examples/zip/zip.nail#L49}{\yes} &
\href{https://android.googlesource.com/platform/system/core/+/refs/tags/android-9.0.0_r1/libziparchive/zip_archive.cc#594}{\no} &
\href{https://github.com/madler/sunzip/blob/v0.4/sunzip.c#L1268-L1269}{\yes}
\\
\thead[r]{tolerates incorrect \CRC} &
\maybe{warns} &
\href{https://github.com/python/cpython/blob/v3.7.0/Lib/zipfile.py#L893-L894}{\no} &
\href{https://github.com/golang/go/blob/go1.12/src/archive/zip/reader.go#L219-L224}{\maybe{if zero}} &
\href{https://github.com/thejoshwolfe/yauzl/tree/2.10.0#no-crc-32-checking}{\yes} &
\href{https://github.com/jbangert/nail/blob/4bd9cc29c4092abe7a77f8294aff2337bba02ec5/examples/zip/zip.nail#L41}{\no} &
\href{https://android.googlesource.com/platform/system/core/+/refs/tags/android-9.0.0_r1/libziparchive/zip_archive.cc#52}{\yes} &
\href{https://github.com/madler/sunzip/blob/v0.4/sunzip.c#L1465-L1469}{\no}
\\
\thead[r]{permits file size of $2^{32}-1$} &
\yes &
\href{https://github.com/python/cpython/blob/v3.7.0/Lib/zipfile.py#L1311-L1313}{\yes} &
\href{https://github.com/golang/go/blob/go1.12/src/archive/zip/reader.go#L406-L414}{\yes} &
% ANON
\no & % \href{https://github.com/thejoshwolfe/yauzl/issues/109}{\no} &
\href{https://github.com/jbangert/nail/blob/4bd9cc29c4092abe7a77f8294aff2337bba02ec5/examples/zip/zip.nail#L59}{\yes} &
\href{https://android.googlesource.com/platform/system/core/+/refs/tags/android-9.0.0_r1/libziparchive/zip_archive_common.h#95}{\yes} &
\href{https://github.com/madler/sunzip/blob/master/sunzip.c#L1275-L1277}{\yes}
\\
\thead[r]{permits file count of $2^{16}-1$} &
\yes &
\href{https://github.com/python/cpython/blob/v3.7.0/Lib/zipfile.py#L258-L259}{\yes} &
\href{https://github.com/golang/go/blob/go1.12/src/archive/zip/reader.go#L502-L511}{\yes} &
% ANON
\no & % \href{https://github.com/thejoshwolfe/yauzl/issues/108}{\no} &
\href{https://github.com/jbangert/nail/blob/4bd9cc29c4092abe7a77f8294aff2337bba02ec5/examples/zip/zip.nail#L79}{\yes} &
\href{https://android.googlesource.com/platform/system/core/+/refs/tags/android-9.0.0_r1/libziparchive/zip_archive_common.h#51}{\yes} &
\href{https://github.com/madler/sunzip/blob/master/sunzip.c#L1139}{\yes}
\\
\noalign{\vspace{0.5em}}
\thead[r]{parses full overlap\\(\autoref{sec:overlap})} &
\maybe{warns} &
\no &
\yes &
\yes &
\yes &
\no &
\no
\\
\thead[r]{parses quoted overlap\\(\autoref{sec:quote})} &
\yes &
\yes &
\yes &
\yes &
\yes &
\yes &
\no
\\
\thead[r]{parses quoted overlap Zip64\\(\autoref{sec:zip64})} &
\yes &
\yes &
\yes &
\yes &
\no &
\no &
\no
\end{tabular}
\caption{
Compatibility of selected zip parsers with various zip features,
edge cases,
and zip bomb constructions.
For best compatibility,
use DEFLATE compression without Zip64,
match names in central directory headers and local file headers,
compute correct CRCs,
and avoid the maximum values of 32-bit and 16-bit fields.
}
\label{tab:compatibility}
\end{table*}


\section{Discussion}
\label{sec:discussion}

We have designed the zip bomb for compatibility,
taking into consideration a number of implementation differences,
some of which are shown in \autoref{tab:compatibility}.
The resulting construction is compatible with zip parsers that work
in the usual back-to-front way,
that first consult the central directory
and use it as an index of files.
This includes, for example,
zip parser included in Nail~\cite{186219},
which is automatically generated from a formal grammar.
The construction is not compatible, however,
with ``streaming'' parsers,
those that parse the zip file in one pass
without first reading the central directory,
reading from the beginning of the file and looking for local file headers.
By their nature, streaming parsers
do not support any kind of file overlapping.
The most likely outcome is that a streaming parser
will extract only the first file.
They may even raise an error besides,
as is the case with \mbox{sunzip}~\cite{sunzip},
which parses the central directory and checks it for consistency
after having extracted the files.

It is easy to detect the type of zip bomb
developed in this article.
Just look for overlapping files.
Parse the central directory,
then sort the central directory entries
in order of the local file headers the point to.
Reject the zip file if any file's ``compressed size''
metadata field would make it overlap the following file.
In general, though, rejecting overlapping files is not sufficient.
\autoref{sec:bzip2} and \autoref{fig:zipped-size}
show that at small sizes, a non-overlapped bzip2 zip bomb
is even more effective than
the overlapped construction with DEFLATE.
Trying to predict a zip file's total uncompressed size
by peeking into the metadata and summing
the ``uncompressed size'' field of every file
does not work, in general,
because the value of the metadata field
may not agree with the actual uncompressed size of the file.
In general, trying to predict in advance whether a zip file
is safe to open or not is fraught,
because there are so many zip parser implementations
with differing behavior.
Robust protection against zip bombs
involves placing time, memory, and disk space limits
on the zip parser while it operates.


% ANON
% \section*{Acknowledgements}


\section*{Availability}

\begin{quote}
\sl
For anonymous submission,
we have uploaded source code
and the three example zip bombs
to an anonymous hosting service.
We will of course make the source code
and zip bombs publicly available.
Do be careful with these---at least,
be sure to ``Save As''
to avoid automatic unzipping by your web browser.
\end{quote}

\begin{description}
\item[source.zip]
{\tiny\texttt{SHA256:TODO}}\\
\url{TODO}
\item[zbsm.zip]
{\tiny\texttt{SHA256:fb4ff972d21189beec11e05109c4354d0cd6d3b629263d6c950cf8cc3f78bd99}}\\
\url{TODO}
\item[zblg.zip]
{\tiny\texttt{SHA256:f1dc920869794df3e258f42f9b99157104cd3f8c14394c1b9d043d6fcda14c0a}}\\
\url{TODO}
\item[zbxl.zip]
{\tiny\texttt{SHA256:eafd8f574ea7fd0f345eaa19eae8d0d78d5323c8154592c850a2d78a86817744}}\\
\url{TODO}
\end{description}

% TODO: https://framadrop.org/

% ANON
% \url{https://www.bamsoftware.com/hacks/zipbomb/}
% also paper sources

\bibliographystyle{plain}
\bibliography{zipbomb}

\end{document}
