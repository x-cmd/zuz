\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix2019_v3}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fancyvrb}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{pifont}
\usepackage{threeparttable}
\usepackage{xcolor}
\usepackage[binary-units,number-unit-product=~]{siunitx}

\urlstyle{same}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
% Better look for citations that include a section reference like \cite[\S 3]{foobar}.
\renewcommand{\citemid}{~}

% Disable metadata for reproducible PDF.
% https://tex.stackexchange.com/a/313605
\ifpdf
\pdfinfoomitdate=1
\pdftrailerid{}
\pdfsuppressptexinfo=-1
\hypersetup{pdfcreator={},pdfproducer={}}
\fi

\newcommand{\kB}{\kilo\byte}
\newcommand{\MB}{\mega\byte}
\newcommand{\GB}{\giga\byte}
\newcommand{\TB}{\tera\byte}
\newcommand{\PB}{\peta\byte}
\newcommand{\EB}{\exa\byte}
\newcommand{\EiB}{\exbi\byte}

\newcommand{\CDH}{\mathrm{CDH}}
\newcommand{\LFH}{\mathrm{LFH}}
\newcommand{\Q}{\mathrm{Q}}
\newcommand{\C}{\mathrm{C}}
\newcommand{\OPT}{\mathrm{OPT}}
\newcommand{\bulkdeflate}{\mbox{bulk\_deflate}}
\newcommand{\CRC}{\mbox{CRC-32}}

\definecolor{ycolor}{HTML}{fcf192}
\definecolor{mcolor}{HTML}{dfe280}
\definecolor{ncolor}{HTML}{94a7ca}
\newcommand{\yes}{\cellcolor{ycolor}\checkmark}
\newcommand{\maybe}[1]{\cellcolor{mcolor}#1}
\newcommand{\no}{\cellcolor{ncolor}\ding{55}}

\begin{document}

\date{}

\title{\Large \bf A better zip bomb}

\author{
{\rm David Fifield}
}

\maketitle

\begin{abstract}
We show how to construct a
\emph{non-recursive} zip bomb
that achieves a high compression ratio by
overlapping files inside the zip container.
``Non-recursive'' means that it does not rely on
a decompressor's recursively unpacking zip files nested within zip files:
it expands fully after a single round of decompression.
The output size increases quadratically in the input size,
reaching a compression ratio of over 28~million
($\SI{10}{\MB} \rightarrow \SI{281}{\TB}$)
at the limits of the zip format.
Even greater expansion is possible using
64-bit extensions.
The construction uses only the most common compression algorithm, DEFLATE,
and is compatible with most zip parsers.
\end{abstract}


\section{Introduction}
\label{sec:intro}

Compression bombs that use the zip format
must cope with the fact that DEFLATE,
the compression algorithm most commonly supported by zip parsers,
cannot achieve a compression ratio greater than
\num{1032}~\cite{zlib_tech}.
For this reason, zip bombs typically rely on recursive decompression,
nesting zip files within zip files to get an extra factor of 1032 with each layer.
But the trick only works on implementations that
unzip recursively, and most do not.
The best-known zip bomb, 42.zip~\cite{42.zip},
expands to a formidable \SI{4.5}{\PB}
if all six of its layers are recursively unzipped,
but a trifling \SI{0.6}{\MB} at the top layer.
Zip quines, like those of
Ellingsen~\cite{ellingsen}
and Cox~\cite{cox},
which contain a copy of themselves
and thus expand infinitely if recursively unzipped,
are likewise perfectly safe to unzip once.

This article shows how to construct a non-recursive zip bomb
whose compression ratio surpasses the DEFLATE limit of 1032.
It works by overlapping files inside the zip container,
in order to reference a ``kernel'' of highly compressed data
in multiple files, without making multiple copies of it.
The zip bomb's output size grows quadratically in the input size; i.e.,
the compression ratio gets better as the bomb gets bigger.
The construction depends on features of both zip and DEFLATE---it
is not directly portable to other file formats or compression algorithms.
It is compatible with most zip parsers,
the exceptions being ``streaming'' parsers that
parse in one pass without first consulting the zip file's central directory.
We try to balance
two conflicting goals:
\begin{itemize}
\item
Maximize the compression ratio.
We define the compression ratio as the the sum of the sizes
of all the files contained the in the zip file,
divided by the size of the zip file itself.
It does not count filenames or other filesystem metadata,
only contents.
\item
Be compatible.
Zip is a tricky format and parsers differ, especially
around edge cases and optional features.
Avoid taking advantage of tricks that only work with certain parsers.
We will remark on certain ways to increase the efficiency of the zip bomb
that come with some loss of compatibility.
\end{itemize}

The construction we will develop is tunable for different sizes.
For the sake of comparison and discussion,
we will produce three concrete examples.
These examples and others are compared in \autoref{tab:comparison}
and \autoref{fig:zipped-size}.

\begin{center}
\begin{tabular}{lr@{~}l@{${} \rightarrow {}$}r@{~}ll}
zbsm.zip & \num{42} & kB & \num{5.5}   & GB & \autoref{sec:allocation} \\
zblg.zip & \num{10} & MB & \num{281.4} & TB & \autoref{sec:allocation} \\
zbxl.zip & \num{46} & MB & \num{4.5}   & PB & \autoref{sec:zip64}
\end{tabular}
\end{center}


\section{Structure of a zip file}
\label{sec:zipstructure}

A~zip file consists of
a \emph{central directory} which references
\emph{files}.
Refer to \autoref{fig:normal}.

The central directory is at the end of the zip file.
It is a list of \emph{central directory headers}.
Each central directory header contains metadata for a single file,
like its filename and \CRC\ checksum,
and a backwards pointer to a local file header.
A~central directory header is \SI{46}{bytes} long,
plus the length of the filename.

A~file consists of a \emph{local file header}
followed by compressed \emph{file data}.
The local file header is \SI{30}{bytes} long,
plus the length of the filename.
It contains a redundant copy
of the metadata from the central directory header,
and the compressed and uncompressed sizes of the file data
that follows.
Zip is a container format, not a compression algorithm.
Each file's data is compressed
using an algorithm specified in the metadata---usually DEFLATE~\cite{rfc1951}.

This description of the zip format omits many details that
are not needed for understanding the zip bomb.
For full information,
refer to the file format specification~\cite{appnote},
particularly Section~4.3.

\begin{table*}
\centering
\begin{threeparttable}
\setlength{\tabcolsep}{0.45em}
\begin{tabular}{rr|rr@{}l|rr@{}l}
&
&
\multicolumn{3}{c|}{\textbf{non-recursive}} &
\multicolumn{3}{c}{\textbf{recursive}}
\\
&
zipped size &
unzipped size &
\multicolumn{2}{r|}{ratio\phantom{~thousand}} &
unzipped size &
\multicolumn{2}{r}{ratio\phantom{~thousand}}
\\
Cox quine~\cite{cox} &
\num{440} &
\num{440} &
\num{1}&.0 &
$\infty$ &
$\infty$&
\\
Ellingsen quine~\cite{ellingsen} &
\num{28809} &
\num{42569} &
\num{1}&.5 &
$\infty$ &
$\infty$&
\\
42.zip~\cite{42.zip} &
\num{42374}\tnote{*} &
\num{558432} &
\num{13}&.2 &
\num{4507981343026016} &
106&~billion
\\
zbsm.zip (this method) &
\num{42374} &
\num{5461307620} &
129&~thousand &
\num{5461307620} &
129&~thousand
\\
zblg.zip (this method) &
\num{9893525} &
\num{281395456244934} &
28&~million &
\num{281395456244934} &
28&~million
\\
zbxl.zip (this method, Zip64)&
\num{45876952} &
\num{4507981427706459} &
98&~million &
\num{4507981427706459} &
98&~million
% \\
% zbxxl.zip &
% \num{2961656712} &
% \num{18446744085437447493} &
% 6&~billion &
% \num{18446744085437447493} &
% 6&~billion
\end{tabular}
\caption{
Comparison of zip bomb compression ratios.
}
\label{tab:comparison}
\begin{tablenotes}
\item [*]
There are two versions of 42.zip,
an \href{https://web.archive.org/web/20120222083624/http://www.unforgettable.dk/}{older version} of \num{42374} bytes,
and a \href{https://web.archive.org/web/20120301154142/http://www.unforgettable.dk/}{newer version} of \num{42838} bytes.
The difference is that the newer version requires a password before unzipping.
We compare only against the older version.
\end{tablenotes}
\end{threeparttable}
\end{table*}

\begin{figure*}
\includegraphics{figures/normal}
\caption{
A normal zip file
(\autoref{sec:zipstructure}).
}
\label{fig:normal}
\end{figure*}


\section{The first insight: overlapping files}
\label{sec:overlap}

By compressing a long string of repeated bytes,
we can produce a \emph{kernel}
of highly compressed data.
By itself, the kernel's compression ratio cannot
exceed the DEFLATE limit of \num{1032},
so we want a way to reuse the kernel in many files,
without making a separate copy of it in each file.
We can do it by overlapping files:
making many central directory headers point to
a single file, whose data is the kernel.
See \autoref{fig:overlap}.

\begin{figure*}
\includegraphics{figures/overlap}
\caption{
Full-overlap zip bomb construction
(\autoref{sec:overlap}).
This construction has problems with compatibility,
because filenames do not agree between the
central directory headers and the local file headers.
The ``kernel'' is a block of highly compressed data,
reused in every file.
}
\label{fig:overlap}
\end{figure*}

Let's look at an example to see how this construction affects the compression ratio.
Suppose the kernel is \SI{1000}{bytes} and
decompresses to \SI{1}{\MB}.
Then the first \SI{1}{\MB} of output ``costs''
\SI{1078}{bytes} of input:
\SI{31}{bytes} for a local file header (including a 1-byte filename),
\SI{47}{bytes} for a central directory header (including a 1-byte filename), and
\SI{1000}{bytes} for the kernel itself.
But every \SI{1}{\MB} of output
after the first costs only \SI{47}{bytes}---we don't need another local file header or another copy of the kernel,
only an additional central directory header.
So while the first copy of the kernel has a compression ratio of
$\num{1000000}/\num{1078}\approx \num{928}$,
each additional copy pulls the ratio closer to
$\num{1000000}/\num{47}\approx \num{21277}$.
A~bigger kernel raises the ceiling.

The problem with this idea is a lack of compatibility.
Because many central directory headers point to a single local file header,
the metadata---specifically the filename---cannot match for every file.
Some parsers balk at that; see \autoref{tab:compatibility}.
Info-ZIP UnZip~\cite{infozip-unzip}
(the standard Unix \texttt{unzip} program)
extracts the files, but with warnings:

{\small
\begin{Verbatim}[commandchars=\\\{\}]
$ \textbf{unzip overlap.zip}
  inflating: A
B:  mismatching "local" filename (A),
         continuing with "central" filename version
  inflating: B
\textit{...}
\end{Verbatim}
}

\noindent
And the Python zipfile module~\cite{python-zipfile}
throws an exception:

{\small
\begin{Verbatim}[commandchars=\\\{\}]
$ \textbf{python3 -m zipfile -e overlap.zip .}
Traceback (most recent call last):
\textit{...}
__main__.BadZipFile: File name in directory 'B' \textbackslash
and header b'A' differ.
\end{Verbatim}
}

Next we will see how to modify the construction
for consistency of filenames,
while still retaining most of the advantage
of overlapping files.

% If you are targeting a specific zip parser that you know
% does not check for filename consistency,
% this construction is a good one.
% It grows faster (as a function of the input size)
% than the construction of the next section,
% which has better compatibility
% because it uses separate local file headers.


\section{The second insight:\texorpdfstring{\\}{ }quoting local file headers}
\label{sec:quote}

We need to separate the local file headers for each file,
while still reusing a single kernel.
Simply concatenating all the local file headers does not work,
because the zip parser will find a local file header
where it expects to find the beginning of a DEFLATE stream.
But the idea will work, with a minor modification.
We'll use a feature of DEFLATE, non-compressed blocks,
to ``quote'' local file headers
so that they appear to be part of the same DEFLATE stream
that terminates in the kernel.
Every local file header
(except the first)
will be interpreted in two ways:
as code (part of the structure of the zip file)
and as data (part of the contents of a file).

A~DEFLATE stream is a sequence of
blocks~\cite[\S 3.2.3]{rfc1951},
where each block may be compressed or non-compressed.
Compressed blocks are what we usually think of;
for example the kernel is one big compressed block.
But there are also non-compressed blocks,
which start with a
5-byte header~\cite[\S 3.2.4]{rfc1951}
that means simply, ``output the next $n$ bytes verbatim.''
Decompressing a non-compressed block means only stripping the 5-byte header.
Compressed and non-compressed blocks may be intermixed freely
in a DEFLATE stream.
The output is the concatenation of
decompressing all the blocks in order.
The ``non-compressed'' notion only has meaning at the DEFLATE layer;
the file data still counts as ``compressed'' at the zip layer,
no matter what kind of blocks are used.

It is easiest to understand this quoted-overlap construction from the inside out,
beginning with the last file and working backwards to the first.
Refer to \autoref{fig:quote}.
Start by inserting the kernel, which will form the end of file data for every file.
Prepend a local file header $\LFH_N$
and add a central directory header $\CDH_N$ that points to it.
Set the ``compressed size'' metadata field in $\LFH_N$ and $\CDH_N$ to the compressed size of the kernel.
Now, insert before $\LFH_N$ a 5-byte non-compressed block header (colored green in the diagram)
whose length field is equal to the size of $\LFH_N$.
Prepend a second local file header $\LFH_{N-1}$
and add a central directory header $\CDH_{N-1}$ that points to it.
Set the ``compressed size'' metadata field in both of the new headers to the compressed size of the kernel,
\emph{plus} the size of the non-compressed block header (\SI{5}{bytes}),
\emph{plus} the size of $\LFH_N$.

\begin{figure*}
\includegraphics{figures/quote}
\caption{
Quoted-overlap zip bomb construction
(\autoref{sec:quote}).
Each file contains the local file headers of all the files which follow it,
as well as the kernel.
The green parts stand for DEFLATE non-compressed blocks.
}
\label{fig:quote}
\end{figure*}

At this point the zip file contains two files, named ``Y'' and ``Z''.
Let's walk through what a zip parser would see while parsing it.
Suppose the compressed size of the kernel is \SI{1000}{bytes}
and the size of $\LFH_N$ is \SI{31}{bytes}.
We start at $\CDH_{N-1}$
and follow the pointer to $\LFH_{N-1}$.
The first file's filename is ``Y'' and
the compressed size of its file data is \SI{1036}{bytes}.
Interpreting the next \SI{1036}{bytes} as a DEFLATE stream,
we first encounter the 5-byte header of a non-compressed block
that says to copy the next \SI{31}{bytes}.
We write the next \SI{31}{bytes},
which are $\LFH_N$,
as output to the file ``Y''.
Moving on in the DEFLATE stream, we find a compressed block (the kernel),
which we decompress and append to file ``Y''.
Now we have reached the end of the compressed data and are done with file ``Y''.
Proceeding to the next file, we follow the pointer from $\CDH_N$
to $\LFH_N$ and find a file named ``Z''
whose compressed size is \SI{1000}{bytes}.
Interpreting those \SI{1000}{bytes} as a DEFLATE stream,
we immediately encounter a compressed block (the kernel again)
and decompress it to the file ``Z''.
Now we have reached the end of the final file and are done.
The output file ``Z'' contains the decompressed kernel;
the output file ``Y'' is the same, but additionally prefixed by
the \SI{31}{bytes} of
$\LFH_N$.

We complete the construction by repeating the quoting procedure
until the zip file contains the desired number of files.
Each new file adds a central directory header,
a local file header,
and a non-compressed block to quote the immediately succeeding local file header.
Compressed file data is generally a chain of DEFLATE non-compressed blocks
(the quoted local file headers)
followed by the compressed kernel.
Each byte in the kernel contributes about
$\num{1032}N$ to the output size,
because each byte is part of all $N$ files.
The output files are not all the same size:
those that appear earlier in the zip file
are larger than those that appear later,
because they contain more quoted local file headers.
The contents of the output files are not particularly meaningful,
but no one said they had to make sense.

This quoted-overlap construction has better compatibility
than the full-overlap construction of \autoref{sec:overlap},
but the compatibility comes at the expense of the compression ratio.
There, each added file cost only a central directory header;
here, it costs a central directory header,
a local file header,
and another \SI{5}{bytes} for the quoting header.


\section{Optimization}
\label{sec:optimization}

Now that we have the basic zip bomb construction,
we will try to make it as efficient as possible.
We want to answer two questions:

\begin{itemize}
\item For a given zip file size, what is the maximum compression ratio?
\item What is the maximum compression ratio, given the limits of the zip format?
\end{itemize}

\subsection{Kernel compression}
\label{sec:bulkdeflate}

It pays to compress the kernel as densely as possible,
because every decompressed byte gets magnified by a factor of $N$.
To that end, we use a custom DEFLATE compressor
called \bulkdeflate,
specialized for compressing
a string of repeated bytes.

%          engine compressed_size max_uncompressed_size
% 1: bulk_deflate           21090              21749401
% 2:         zlib           21090              21723602
% 3:       zopfli           21090              21734018

All decent DEFLATE compressors will approach a compression ratio of \num{1032}
when given an infinite stream of repeating bytes,
but we care more about specific finite sizes
than asymptotics.
\autoref{fig:max-uncompressed-size} shows the
maximum amount of output
that can result from decompressing a DEFLATE stream of a given size,
for \bulkdeflate\ and other implementations.
\bulkdeflate\ compresses more data
into the same space than the general-purpose compressors:
about \SI{26}{\kB} more than zlib and Info-ZIP,
and about \SI{15}{\kB} more than Zopfli~\cite{zopfli},
a compressor that trades speed for density.

\begin{figure}
\includegraphics{data/max_uncompressed_size}
\caption{
Comparison of DEFLATE compressors
on a string of repeated bytes.
The axes are chosen to show the neighborhood
of the kernel of zbsm.zip, which is developed in \autoref{sec:allocation}.
}
\label{fig:max-uncompressed-size}
\end{figure}

The price of \bulkdeflate's high compression ratio is a lack of generality.
\bulkdeflate\ can only compress strings of a single repeated byte,
and only those of specific lengths,
namely $517 + 258 k$ for integer $k \ge 0$.
Besides compressing densely, \bulkdeflate\ is fast,
doing essentially constant work regardless of the input size,
aside from the $O(n)$ work of actually writing out the compressed string.

\subsection{Filenames}
\label{sec:filenames}

For our purposes, filenames are mostly dead weight.
While filenames do contribute something to the output size
by virtue of being part of quoted local file headers,
a byte in a filename does not contribute nearly as much
as a byte in the kernel.
We want filenames to be as short as possible,
while keeping them all distinct,
and subject to compatibility considerations.

The first compatibility consideration is character encoding.
The zip format specification states that filenames
are to be interpreted as CP~437,
or \mbox{UTF-8} if a certain flag bit is set~\cite[Appendix~D]{appnote}.
But this is a major point of incompatibility
across zip parsers,
which may interpret filenames as being in
some fixed or locale-specific encoding.
So for compatibility, we must limit ourselves to characters
that have the same encoding in both
CP~437 and \mbox{UTF-8};
namely, the \num{95} printable characters of \mbox{US-ASCII}.
% https://bugs.python.org/issue10614
% https://bugs.python.org/issue10972
% https://github.com/thejoshwolfe/yauzl/issues/84

We are further restricted by filesystem naming limitations.
Some filesystems are case-insensitive, so ``a'' and ``A'' do not count as distinct names.
Common filesystems like FAT32 prohibit certain characters like
`*' and `?'~\cite[\S Limits]{wiki-fs}.

As a safe but not necessarily optimal compromise,
our zip bomb will use filenames consisting of characters
drawn from a 36-character alphabet
that does not
rely on case distinctions
or use special characters:

\begin{center}
\setlength{\tabcolsep}{2pt}
\begin{tabular}{cccccccccccccccccc}
0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & A & B & C & D & E & F & G & H \\
I & J & K & L & M & N & O & P & Q & R & S & T & U & V & W & X & Y & Z
\end{tabular}
\end{center}

\noindent
Filenames are generated in the obvious way,
cycling each position through the possible characters
and adding a position on overflow:

\begin{center}
\setlength{\tabcolsep}{2pt}
\begin{tabular}{rrrrrl}
``\mbox{0}'', &
``\mbox{1}'', &
``\mbox{2}'', &
\ldots, &
``\mbox{Z}'',
\\
``\mbox{00}'', &
``\mbox{01}'', &
``\mbox{02}'', &
\ldots, &
``\mbox{0Z}'',
\\
\ldots,
\\
``\mbox{Z0}'', &
``\mbox{Z1}'', &
``\mbox{Z2}'', &
\ldots, &
``\mbox{ZZ}'',
\\
``\mbox{000}'', &
``\mbox{001}'', &
``\mbox{002}'', &
\ldots
\end{tabular}
\end{center}

\noindent
There are $36$ filenames of length~\num{1},
$36^2$ filenames of length~\num{2}, and so on.
The length of the $n$\/th filename is
$\lfloor \log_{36}((n + 1) / \frac{36}{35})\rfloor + 1 = O(\log n)$.
Four bytes are enough to represent
\num{1727604} distinct filenames.

% The sum of the lengths of the first $n$ filenames is
% $dn - ((36^d - 1) \frac{36}{35^2} - \frac{d}{35})$,
% where $d = \lfloor \log_{36}(n / \frac{36}{35})\rfloor$.
% https://oeis.org/A014824

Given that the $N$ filenames in the zip file
are generally not all of the same length,
which way should we order them,
shortest to longest or longest to shortest?
A~little reflection shows that it is better to
put the longest names last, because those names are the most quoted.
Ordering filenames longest last
adds over \SI{900}{\MB} of output
to the \mbox{zblg.zip} we will see in \autoref{sec:allocation},
compared to ordering them longest first.
It is a minor optimization, though,
as those \SI{900}{\MB} comprise only \num{0.0003}\%
of the total output size.

% $ unzip -l zblg.zip | tail -n 1
% 281395456244934                     65534 files
% $ unzip -l zblg.rev.zip | tail -n 1
% 281394526372494                     65534 files
% $ python
% >>> 281395456244934 - 281394526372494
% 929872440
% >>> 929872440 / 281395456244934. * 100
% 0.00033045041039703735

\subsection{Kernel size}
\label{sec:allocation}

The quoted-overlap construction
allows us to place a compressed kernel of data,
and then cheaply copy it many times.
For a given zip file size~$X$,
how much space should we devote to storing the kernel,
and how much to making copies?

To find the optimum balance,
we only have to optimize the single variable $N$,
the number of files in the zip file.
Every value of $N$ requires
a certain amount of overhead for
central directory headers,
local file headers,
quoting block headers, and filenames.
All the remaining space can be taken up by the kernel.
Because $N$ has to be an integer,
and you can only fit so many files
before the kernel size drops to zero,
it suffices to test every possible value of $N$
and select the one that yields the most output.

Applying the optimization procedure to $X = \num{42374}$,
the size of 42.zip,
finds a maximum at $N = 250$.
Those \num{250} files require \SI{21195}{bytes} of overhead,
leaving \SI{21179}{bytes} for the kernel.
A~kernel of that size decompresses to \SI{21841249}{bytes}
(a~ratio of \num{1031.3}).
The \num{250} copies of the decompressed kernel,
plus the little bit extra that comes from the quoted local file headers,
produces an overall unzipped output of
\SI{5461307620}{bytes}
and a~compression ratio of 128~thousand.
This zip bomb is \mbox{zbsm.zip}---refer to \autoref{tab:comparison}.

Optimization produced an almost even split
between the space allocated to the kernel
and the space allocated to file headers.
It is not a coincidence.
Let's look at a simplified model of the quoted-overlap construction.
In the simplified model,
we ignore filenames,
as well as the slight increase in output file size
due to quoting local file headers.
Analysis of the simplified model will show that the optimum
split between kernel and file headers is approximately even,
and that the output size grows quadratically
when allocation is optimal.

Define some constants and variables:

\begin{align*}
X & & & \mbox{zip file size (take as fixed)} \\
N & & & \mbox{number of files (variable to optimize)} \\
\CDH &= \num{46} & & \mbox{size of a central directory header} \\
\LFH &= \num{30} & & \mbox{size of a local file header} \\
\Q   &=  \num{5} & & \mbox{size of a quoting block header} \\
\C   &\approx \num{1032} & & \mbox{compression ratio of the kernel}
\end{align*}

% JACAL session to do the algebra and calculus:
% 
% # S_X(N)
% e1 : C * N * (X - (N * (LFH + CDH) + (N - 1) * Q)));
%                                                   ^
% 
%                        2             2
% e1: (- C CDH - C LFH) N  + (C N - C N ) Q + C N X
% 
% # S_X(N) as a polynomial in N
% e2 : coeffs(e1, N);
% 
% e2: [0, C Q + C X, - C CDH - C LFH - C Q]
% 
% # S'_X(N)
% e3 : diff(e1, N);
% 
% e3: (- 2 C CDH - 2 C LFH) N + (C - 2 C N) Q + C X
% 
% # S'_X(N) as a polynomial in N
% e4 : coeffs(e3, N);
% 
% e4: [C Q + C X, - 2 C CDH - 2 C LFH - 2 C Q]
% 
% # Solve S'_X(N) = 0. e5 == N_OPT
% e5 : suchthat(N, e3);
% 
%            Q + X
% e5: - - - - - - - - - -
%     2 CDH + 2 LFH + 2 Q
% 
% # H(N_OPT)
% e6 : e5 * (LFH + CDH) + (e5 - 1) * Q;
% 
%     - Q + X
% e6: - - - -
%        2
% 
% # S_X(N_OPT)
% e7 : -C * (CDH + LFH + Q) * e5^2 + C * (Q + X) * e5;
% 
%        2                2
%     C Q  + 2 C Q X + C X
% e7: - - - - - - - - - - -
%      4 CDH + 4 LFH + 4 Q

Let $H(N)$
be the amount of header overhead required for $N$ files.
Refer to \autoref{fig:quote}
to understand where this formula comes from.

\begin{align*}
H(N) &= N\cdot(\CDH + \LFH) + (N - 1)\cdot\Q
\end{align*}

The space remaining for the kernel is
$X - H(N)$.
The total unzipped size
$S_X(N)$
is the size of $N$ copies
of the kernel,
decompressed at ratio~$\C$.
(In this simplified model we ignore
the minor additional expansion from quoted local file headers.)

\begin{align*}
S_X(N) &= (X - H(N)) \, \C \, N \\
       &= (X - (N \cdot (\CDH + \LFH) + (N - 1)\cdot\Q)) \, \C \, N \\
       &= -(\CDH + \LFH + \Q) \, \C \, N^2 + (X + \Q) \, \C \, N
\end{align*}

$S_X(N)$
is a polynomial in $N$,
so its maximum must be at a place where the derivative
$S'_X(N)$
is zero.
Taking the derivative and finding the zero gives us
$N_\OPT$,
the optimal number of files.

\begin{align*}
S'_X(N_\OPT) &= - 2 (\CDH + \LFH + \Q) \, C \, N_\OPT + (X + \Q) \, \C \\
           0 &= - 2 (\CDH + \LFH + \Q) \, C \, N_\OPT + (X + \Q) \, \C \\
     N_\OPT  &= \frac{X + \Q}{2 (\CDH + \LFH + \Q)}
\end{align*}

$H(N_\OPT)$
gives the optimal amount of space to allocate for file headers.
It is independent of $\CDH$, $\LFH$, and $\C$,
and is close to $X/2$.

\begin{align*}
H(N_\OPT) &= N_\OPT\cdot(\CDH + \LFH) + (N_\OPT - 1)\cdot\Q \\
          &= \frac{X - \Q}{2}
\end{align*}

$S_X(N_\OPT)$
is the total unzipped size
when the allocation is optimal.
From this we see that the output size grows quadratically
in the input size.

\begin{align}
\label{eq:opt}
S_X(N_\OPT) &= \frac{(X + \Q)^2 \, \C}{4(\CDH + \LFH + \Q)}
\end{align}

As we make the zip file larger,
eventually we run into the limits of the zip format.
A~zip file can contain at most $2^{16}-1$ files,
and each file can have an uncompressed size of at most $2^{32}-1$ bytes.
Worse than that,
some implementations (see \autoref{tab:compatibility})
take the maximum possible values
as an indicator of the presence of 64-bit extensions (\autoref{sec:zip64}),
so our limits are actually $2^{16}-2$ and $2^{32}-2$.
% https://github.com/golang/go/commit/4aedbf5be4631693f774063410707ef467ca78e7
% https://github.com/golang/go/commit/b6c5edae7c0e9dd6d12dbb8f1c9638dea45f9464
It happens that the first limit we hit is the one on uncompressed file size.
At a zip file size of \SI{8319377}{bytes},
naive optimization would give us a file count of \num{47837}
and a largest file of
$2^{32}+311$ bytes.
% $compressed_size
% [1] 4160277
% 
% $num_files
% [1] 47837
% 
% [1] "zipped size" "8319377"
% [1] "unzipped size"   "205420672417247"
% [1] 4294967607

Accepting that we cannot increase $N$ nor the size of the kernel without bound,
we would like to find the maximum compression ratio achievable
while remaining within the limits of the zip format.
The way to proceed is to make the kernel as large as possible,
and have the maximum number of files.
Even though we can no longer maintain the roughly even split
between kernel and file headers,
each added file \emph{does} increase the compression ratio---just
not as fast as it would if we were able to keep growing the kernel, too.
In fact, as we add files we will need to \emph{decrease} the size of the kernel
to make room for the maximum file size
that gets slightly larger with each added file.

The plan results in \mbox{zblg.zip}, a zip file
that contains $2^{16}-2$ files and a kernel that decompresses
to $2^{32}-\num{2178825}$ bytes.
Refer to \autoref{tab:comparison}.
Files get longer towards the beginning of the zip file---the
first and largest file decompresses to
$2^{32} - 56$ bytes.
That is as close as we can get using the coarse
output sizes of \bulkdeflate---encoding
the final \SI{54}{bytes} would cost more bytes than they are worth.
(The zip file as a whole has a compression ratio
of 28~million, and the final \SI{54}{bytes} would gain
at most $54\cdot 1032\cdot (2^{16}-2) \approx 36.5~\mathrm{million}$ bytes,
so it only helps if the \SI{54}{bytes} can be encoded
in \SI{1}{byte}---we could not do it in less than~\num{2}.)
The output size of this zip bomb, \SI{281395456244934}{bytes},
is 99.97\% of the theoretical maximum
$(2^{32}-1)\cdot(2^{16}-1)$.
% 65 535 × 0xffffffff = <data value="281470681677825">281 470 681 677 825</data>.
Any major improvements to the compression ratio can only come
from reducing the input size,
not increasing the output size.
% >>> (2**32-1)*65535 - 281399752637796
% 70929040029
% >>> 70929040029. / ((2**32-1)*65535)
% 0.00025199441592352524
% >>> 1 - _
% 0.9997480055840765
% >>> _ * 100
% 99.97480055840765


\section{Efficient \CRC\ computation}
\label{sec:crc32}

Among the metadata in the central directory header and local file header
is a \CRC\ checksum of the uncompressed file data.
This poses a problem, because directly calculating the \CRC\ of each file
requires doing work proportional to the total \emph{unzipped} size,
which is large by design. (It's a zip bomb, after all.)
We would prefer to do work that in the worst case is
proportional to the \emph{zipped} size.
Two factors work in our advantage:
all files share a common suffix (the kernel),
and the uncompressed kernel is a string of repeated bytes.
We will represent \CRC\ as a matrix product---this
will allow us not only to compute the checksum of the kernel quickly,
but also to reuse computation across files.
The technique described in this section is a slight extension of the
\texttt{crc32\_combine}
function in zlib,
which Mark Adler has explained~\cite{crc32combine}.

\begin{figure*}
\begin{align*}
M_0 &=
\setcounter{MaxMatrixCols}{33}
\begin{smallmatrix}
0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1
\end{smallmatrix}
&
M_1 &=
\setcounter{MaxMatrixCols}{33}
\begin{smallmatrix}
0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1 \\
0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1 \\
1&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1 \\
0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&0&1 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&0&1 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0&1 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&0&1 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0&1 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&0&1 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0&1 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&0&0 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&0&1 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1&1 \\
1&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1 \\
0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&1
\end{smallmatrix}
\end{align*}
\caption{
The $33\times33$ transformation matrices $M_0$ and $M_1$ that compute
the \CRC\ state change effected by a $0$~bit and a $1$~bit respectively.
Column vectors are stored with the most significant bit at the bottom:
reading the first column from bottom to top, you see
the \CRC\ polynomial constant $\mathrm{edb88320}_{16} = 11101101101110001000001100100000_2$.
The two matrices differ only in the final column, which represents a translation vector
in homogeneous coordinates.
In $M_0$ the translation is zero and
in $M_1$ it is $\mathrm{edb88320}_{16}$, the \CRC\ polynomial constant.
The 1's just above the diagonal represent the
shift operation \texttt{state~>{}>~1}.
}
\label{fig:crc32matrix}
\end{figure*}

You can model \CRC\ as a state machine that updates a 32-bit state register
for each incoming bit.
The basic update operations for a $0$~bit and a $1$~bit are:
{
\small
\begin{verbatim}
uint32 crc32_update_0(uint32 state) {
    // Shift out the least significant bit.
    bit b = state & 1;
    state = state >> 1;
    // If the shifted-out bit was 1, XOR
    // with the CRC-32 constant.
    if (b == 1)
        state = state ^ 0xedb88320;
    return state;
}
uint32 crc32_update_1(uint32 state) {
    // Do as for a 0 bit, then XOR
    // with the CRC-32 constant.
    return crc32_update_0(state) ^ 0xedb88320;
}
\end{verbatim}
}

If you think of the state register as a 32-element binary vector,
and use XOR for addition and AND for multiplication, then
\texttt{crc32\_update\_0} is a linear transformation;
i.e., it can be represented as multiplication by a
$32\times32$ binary transformation matrix.
To see why, observe that multiplying a matrix by a vector
is just summing the columns of the matrix,
after multiplying each column by the corresponding element of the vector.
The shift operation \texttt{state~>{}>~1}
is just taking each bit~$i$ of the state vector
and multiplying it by a vector that is $0$ everywhere except at bit $i-1$
(numbering the bits from right to left).
The conditional final XOR \texttt{state~\^~0xedb88320}
that only happens when bit~\texttt{b} is $1$
can instead be represented as first multiplying
\texttt{b} by 0xedb88320
and then XORing it into the state.

Furthermore, \texttt{crc32\_update\_1} is just
\texttt{crc32\_update\_0} plus (XOR) a~constant.
That makes \texttt{crc32\_update\_1} an
affine transformation:
a~matrix multiplication followed by a translation (i.e., vector addition).
We can represent both the matrix multiplication and the translation
in a single step
if we enlarge the dimensions of the transformation matrix to $33\times33$
and append an extra element to the state vector that is always~$1$.
(This representation is called
homogeneous coordinates.)

Both operations \texttt{crc32\_update\_0} and \texttt{crc32\_update\_1}
can be represented by a $33\times 33$ transformation matrix.
The matrices $M_0$ and $M_1$ are shown in \autoref{fig:crc32matrix}.
The benefit of a matrix representation is that matrices compose.
Suppose we want to represent the state change effected by processing
the ASCII character `a', whose binary representation is
$01100001_2$.
We can represent the cumulative \CRC\ state change of those 8~bits
in a single transformation matrix:

\begin{align*}
M_\mathrm{a} &= M_0 \, M_1 \, M_1 \, M_0 \, M_0 \, M_0 \, M_0 \, M_1
\end{align*}

\noindent
And we can represent the state change of a string of repeated `a's
by multiplying many copies of $M_\mathrm{a}$ together---matrix exponentiation.
We can do matrix exponentiation quickly
using a square-and-multiply algorithm,
which allows us to compute $M^n$
in only about $\log_2 n$ steps.
For example, the matrix representing the state change
of a string of 9~`a's is

\begin{align*}
(M_\mathrm{a})^9 &= M_\mathrm{a} \, M_\mathrm{a} \, M_\mathrm{a} \, M_\mathrm{a} \, M_\mathrm{a} \, M_\mathrm{a} \, M_\mathrm{a} \, M_\mathrm{a} \, M_\mathrm{a} \\
&= (M_\mathrm{a} \, M_\mathrm{a} \, M_\mathrm{a} \, M_\mathrm{a})^2 \, M_\mathrm{a} \\
&= ((M_\mathrm{a} \, M_\mathrm{a})^2)^2 \, M_\mathrm{a} \\
&= (((M_\mathrm{a})^2)^2)^2 \, M_\mathrm{a}
\end{align*}

The square-and-multiply algorithm is useful
for computing $M_\mathrm{kernel}$,
the matrix for the uncompressed kernel,
because the kernel is a string of repeated bytes.
To produce a \CRC\ checksum value from a matrix,
multiply the matrix by the zero vector.
(The zero vector in homogeneous coordinates, that is:
32~$0$'s followed by a $1$.
Here we omit the minor complication of pre- and post-conditioning the checksum.)
To compute the checksum for every file, we work backwards.
Start by initializing $M := M_\mathrm{kernel}$.
The checksum of the kernel is also the checksum
of the final file, file~$N$,
so multiply $M$ by the zero vector and store the resulting checksum in
$\CDH_N$ and $\LFH_N$.
The file data of file~$N-1$ is the same as the file data of file~$N$,
but with an added prefix of $\LFH_N$.
So compute $M_{\LFH_N}$, the state change matrix for $\LFH_N$,
and update $M := M \, M_{\LFH_N}$.
Now $M$ represents the cumulative state change from processing
$\LFH_N$ followed by the kernel.
Compute the checksum for file $N-1$ by again multiplying $M$ by the zero vector.
Continue the procedure, accumulating state change matrices into $M$,
until all checksums have been computed.


\section{Extension: Zip64}
\label{sec:zip64}

In \autoref{sec:allocation} we hit a wall on expansion
due to limits of the zip format---it was impossible
to produce more than about \SI{281}{\TB} of output,
no matter how cleverly packed the zip file.
It is possible to surpass those limits
using Zip64, an extension to the zip format that increases
the size of certain header fields to 64~bits.
Support for Zip64 is by no means universal,
but it is one of the more commonly implemented extensions---see \autoref{tab:compatibility}.
As regards the compression ratio,
the effect of Zip64 is to
increase the size of a central directory header from
\SI{46}{bytes} to \SI{58}{bytes},
and the size of a local directory header from
\SI{30}{bytes} to \SI{50}{bytes}.
Referring to \autoref{eq:opt},
we see that a zip bomb in Zip64 format
still grows quadratically,
but more slowly because of the larger denominator.
In exchange for the loss of compatibility
and slower growth,
we get the removal of all practical file size limits.
\autoref{fig:zipped-size} compares various zip bomb constructions
with and without Zip64.

Suppose we want a zip bomb that expands to \SI{4.5}{\PB},
the same size that 42.zip recursively expands to.
How big must the zip file be?
Using binary search, we find that the smallest
zip file whose unzipped size exceeds the unzipped size of 42.zip
has a zipped size of \SI{46}{\MB}.
See \mbox{zbxl.zip} in \autoref{tab:comparison} and \autoref{fig:zipped-size}.

With Zip64, it's no longer practically interesting to
consider the maximum compression ratio,
because we can just keep increasing the zip file size,
and the compression ratio along with it,
until even the compressed zip file is prohibitively large.
An interesting threshold, though,
is $2^{64}$ bytes
(\SI{18}{\EB} or \SI{16}{\EiB})---that
much data will not fit on most filesystems~\cite[\S Limits]{wiki-fs}.
Binary search finds the smallest zip bomb that produces at least that much output:
it contains 12~million files and has a compressed kernel of \SI{1.5}{\GB}.
The total size of the zip file is \SI{2.9}{\GB} and it unzips
to $2^{64}+\num{11727895877}$ bytes,
having a compression ratio of over 6.2~billion.

\begin{figure}
\includegraphics{data/zipped_size}
\caption{
Zipped size versus unzipped size for various zip bomb constructions.
Note the log--log scales.
Each construction is shown with and without Zip64.
The no-overlap constructions
have a linear rate of growth,
which is visible in the 1:1 slope of the lines.
The vertical offset of the bzip2 lines shows that the compression ratio
of bzip2 is about a thousand times greater than that of DEFLATE.
The quoted-DEFLATE constructions
have a quadratic rate of growth,
as evidenced by the 2:1 slope of the lines.
The Zip64 variant is slightly less efficient,
but permits output in excess of \SI{281}{\TB}.
The lines for extra-field-quoted bzip2
transition from quadratic to linear
upon reaching either the maximum file size ($2^{32}-2$ bytes),
or the maximum number of files allowed by extra-field quoting.
Labeled dots mark specific instances that
appear in \autoref{tab:comparison}.
}
\label{fig:zipped-size}
\end{figure}


\begin{table*}
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{rccccccc}
&
\thead{Info-ZIP\\UnZip~6.0~\cite{infozip-unzip}} &
\thead{Python~3.7\\zipfile~\cite{python-zipfile}} &
\thead{Go~1.12\\archive/zip~\cite{golang-archivezip}} &
\thead{yauzl~2.10.0~\cite{yauzl}\\(Node.js)} &
\thead{Nail~\cite{186219}\\examples/zip~\cite{nail-zip}} &
\thead{Android~9.0.0~r1\\libziparchive~\cite{android-libziparchive}} &
\thead{sunzip~0.4~\cite{sunzip}\\(streaming)}
\\
\thead[r]{DEFLATE} &
\yes &
\href{https://github.com/python/cpython/blob/v3.7.0/Lib/zipfile.py#L57}{\yes} &
\href{https://github.com/golang/go/blob/go1.12/src/archive/zip/struct.go#L31}{\yes} &
\href{https://github.com/thejoshwolfe/yauzl/blob/2.10.0/index.js#L520-L521}{\yes} &
\href{https://github.com/jbangert/nail/blob/4bd9cc29c4092abe7a77f8294aff2337bba02ec5/examples/zip/zip.c#L63}{\yes} &
\href{https://android.googlesource.com/platform/system/core/+/refs/tags/android-9.0.0_r1/libziparchive/zip_archive.cc#1059}{\yes} &
\href{https://github.com/madler/sunzip/blob/v0.4/sunzip.c#L1256}{\yes}
\\
\thead[r]{Zip64} &
\href{http://infozip.sourceforge.net/UnZip.html#Release}{\yes} &
\href{https://github.com/python/cpython/blob/v3.7.0/Lib/zipfile.py#L186}{\yes} &
\href{https://github.com/golang/go/blob/go1.12/src/archive/zip/reader.go#L519}{\yes} &
\href{https://github.com/thejoshwolfe/yauzl/tree/2.10.0#limitted-zip64-support}{\yes} &
\href{https://github.com/jbangert/nail/blob/4bd9cc29c4092abe7a77f8294aff2337bba02ec5/examples/zip/zip.c#L103-L125}{\no} &
\href{https://android.googlesource.com/platform/system/core/+/refs/tags/android-9.0.0_r1/libziparchive/zip_archive.cc#168}{\no} &
\href{https://github.com/madler/sunzip/blob/v0.4/sunzip.c#L922}{\yes}
\\
\thead[r]{bzip2} &
\href{http://infozip.sourceforge.net/UnZip.html#Release}{\yes} &
\href{https://github.com/python/cpython/blob/v3.7.0/Lib/zipfile.py#L58}{\yes} &
\href{https://github.com/golang/go/blob/go1.12/src/archive/zip/struct.go#L28-L32}{\no} &
\href{https://github.com/thejoshwolfe/yauzl/blob/2.10.0/index.js#L517-L525}{\no} &
\href{https://github.com/jbangert/nail/blob/4bd9cc29c4092abe7a77f8294aff2337bba02ec5/examples/zip/zip.c#L86}{\no} &
\href{https://android.googlesource.com/platform/system/core/+/refs/tags/android-9.0.0_r1/libziparchive/zip_archive.cc#1061}{\no} &
\href{https://github.com/madler/sunzip/blob/v0.4/sunzip.c#L1256}{\yes}
\\
\thead[r]{permits mismatched filenames} &
\maybe{warns} &
\href{https://github.com/python/cpython/blob/v3.7.0/Lib/zipfile.py#L1486-L1489}{\no} &
\href{https://github.com/golang/go/blob/go1.12/src/archive/zip/reader.go#L244}{\yes} &
\href{https://github.com/thejoshwolfe/yauzl/tree/2.10.0#local-file-headers-are-ignored}{\yes} &
\href{https://github.com/jbangert/nail/blob/4bd9cc29c4092abe7a77f8294aff2337bba02ec5/examples/zip/zip.nail#L49}{\yes} &
\href{https://android.googlesource.com/platform/system/core/+/refs/tags/android-9.0.0_r1/libziparchive/zip_archive.cc#594}{\no} &
\href{https://github.com/madler/sunzip/blob/v0.4/sunzip.c#L1268-L1269}{\yes}
\\
\thead[r]{permits incorrect \CRC} &
\maybe{warns} &
\href{https://github.com/python/cpython/blob/v3.7.0/Lib/zipfile.py#L893-L894}{\no} &
\href{https://github.com/golang/go/blob/go1.12/src/archive/zip/reader.go#L219-L224}{\maybe{if zero}} &
\href{https://github.com/thejoshwolfe/yauzl/tree/2.10.0#no-crc-32-checking}{\yes} &
\href{https://github.com/jbangert/nail/blob/4bd9cc29c4092abe7a77f8294aff2337bba02ec5/examples/zip/zip.nail#L41}{\no} &
\href{https://android.googlesource.com/platform/system/core/+/refs/tags/android-9.0.0_r1/libziparchive/zip_archive.cc#52}{\yes} &
\href{https://github.com/madler/sunzip/blob/v0.4/sunzip.c#L1465-L1469}{\no}
\\
\thead[r]{permits too-short file size} &
\yes &
\href{https://github.com/python/cpython/blob/v3.7.0/Lib/zipfile.py#L772}{\no} &
\href{https://github.com/golang/go/blob/go1.12/src/archive/zip/reader.go#L205-L207}{\no} &
\href{https://github.com/thejoshwolfe/yauzl/blob/2.10.0/index.js#L641-L655}{\no} &
\href{https://github.com/jbangert/nail/blob/4bd9cc29c4092abe7a77f8294aff2337bba02ec5/examples/zip/zip.nail#L47}{\no} &
\href{https://android.googlesource.com/platform/system/core/+/refs/tags/android-9.0.0_r1/libziparchive/zip_archive.cc#847}{\no} &
\href{https://github.com/madler/sunzip/blob/v0.4/sunzip.c#L1113-L1124}{\no}
\\
\thead[r]{permits file size of $2^{32}-1$} &
\yes &
\href{https://github.com/python/cpython/blob/v3.7.0/Lib/zipfile.py#L1311-L1313}{\yes} &
\href{https://github.com/golang/go/blob/go1.12/src/archive/zip/reader.go#L406-L414}{\yes} &
\href{https://github.com/thejoshwolfe/yauzl/issues/109}{\no} &
\href{https://github.com/jbangert/nail/blob/4bd9cc29c4092abe7a77f8294aff2337bba02ec5/examples/zip/zip.nail#L59}{\yes} &
\href{https://android.googlesource.com/platform/system/core/+/refs/tags/android-9.0.0_r1/libziparchive/zip_archive_common.h#95}{\yes} &
\href{https://github.com/madler/sunzip/blob/master/sunzip.c#L1275-L1277}{\yes}
\\
\thead[r]{permits file count of $2^{16}-1$} &
\yes &
\href{https://github.com/python/cpython/blob/v3.7.0/Lib/zipfile.py#L258-L259}{\yes} &
\href{https://github.com/golang/go/blob/go1.12/src/archive/zip/reader.go#L502-L511}{\yes} &
\href{https://github.com/thejoshwolfe/yauzl/issues/108}{\no} &
\href{https://github.com/jbangert/nail/blob/4bd9cc29c4092abe7a77f8294aff2337bba02ec5/examples/zip/zip.nail#L79}{\yes} &
\href{https://android.googlesource.com/platform/system/core/+/refs/tags/android-9.0.0_r1/libziparchive/zip_archive_common.h#51}{\yes} &
\href{https://github.com/madler/sunzip/blob/master/sunzip.c#L1139}{\yes}
\\
\noalign{\vspace{0.5em}}
\thead[r]{unzips full overlap\\(\autoref{sec:overlap})} &
\maybe{warns} &
\no &
\yes &
\yes &
\yes &
\no &
\no
\\
\thead[r]{unzips quoted overlap\\(\autoref{sec:quote})} &
\yes &
\yes &
\yes &
\yes &
\yes &
\yes &
\no
\\
\thead[r]{unzips quoted overlap Zip64\\(\autoref{sec:zip64})} &
\yes &
\yes &
\yes &
\yes &
\no &
\no &
\no
\end{tabular}
\caption{
Compatibility of selected zip parsers with various zip features,
edge cases,
and zip bomb constructions.
The background colors indicate a scale from \colorbox{ycolor}{less restrictive} to \colorbox{ncolor}{more restrictive}.
For best compatibility,
use DEFLATE compression without Zip64,
match names in central directory headers and local file headers,
compute correct CRCs,
and avoid the maximum values of 16-bit and 32-bit fields.
}
\label{tab:compatibility}
\end{table*}


\section{Extension: bzip2}
\label{sec:bzip2}

% bzip2 starts with a run-length encoding step
% that reduces the length of a string of repeated bytes
% by a factor of~51.
% Then the data is separated into
% <data value="900000">900~<abbr title=kilobyte>kB</abbr></data> blocks
% and each block compressed individually.
% Empirically, one block after run-length encoding
% can compress down to 32~bytes.
% 900 000 × 51 / 32 = 1 434 375.

DEFLATE is the most common compression algorithm
used in the zip format, but it is only one of many options~\cite[\S 4.4.5]{appnote}.
bzip2~\cite{bzip2}, while not as compatible as DEFLATE (see \autoref{tab:compatibility}),
is probably the second most commonly supported compression algorithm.
Empirically, bzip2 has a maximum compression ratio of about 1.4~million,
which allows for denser packing of the kernel.
Ignoring the loss of compatibility,
does bzip2 enable a more efficient zip bomb?

Yes---but only for small files.
The problem is that bzip2 does not have anything like the
non-compressed blocks of DEFLATE
that we used in \autoref{sec:quote} to quote local file headers.
So it is not possible to overlap files and reuse the kernel---each file must have
its own copy, and therefore the overall compression ratio
is no better than the ratio of any single file.
In \autoref{fig:zipped-size} we see that
no-overlap bzip2 outperforms quoted DEFLATE
only for files under about a megabyte.

There is still hope for using bzip2---an
alternative means of local file header quoting
discussed in \hyperref[sec:extra]{the next section}.
Additionally,
if you happen to know that a certain zip parser supports bzip2
\emph{and} tolerates mismatched filenames,
then you can use the full-overlap construction of \autoref{sec:overlap},
which has no need for quoting.


\section{Extension: extra-field quoting}
\label{sec:extra}

In \autoref{sec:quote} we used a feature of
DEFLATE to quote local file headers,
and in \autoref{sec:bzip2} we saw that the same trick
does not work with bzip2.
There is an alternative means of quoting,
somewhat more limited,
that only uses features of the zip format
and does not depend on the compression algorithm.

At the end of the local file header structure
there is a variable-length
\emph{extra field} whose purpose is to store information
that doesn't fit into the ordinary fields of the header~\cite[\S 4.3.7]{appnote}.
The extra information may include, for example,
a high-resolution timestamp or a Unix uid/gid;
Zip64 information is stored in the extra field.
The extra field is represented as a length--value structure;
if we increase the length field without adding to the value,
then the extra field will grow to include whatever
comes after it in the zip file---namely
the next local file header.
Using this technique, each local file header
can ``quote'' the following local file headers
by enclosing them within its own extra field.
The benefits of extra-field quoting,
as compared to DEFLATE non-compressed block quoting,
are threefold:

\begin{enumerate}
\item Extra-field quoting requires only \SI{4}{bytes} of overhead,
not~\num{5}, leaving more room for the kernel.
\item Extra-field quoting does not increase the size of files,
which leaves more headroom for a bigger kernel when
operating at the limits of the zip format.
\item Extra-field quoting provides a way of combining quoting with bzip2.
\end{enumerate}

Despite these benefits, extra-field quoting is less flexible
than DEFLATE quoting.
It does not chain like DEFLATE quoting does:
each local file header must enclose not only the immediately next header
but \emph{all} headers which follow.
The extra fields increase in length as they get closer
to the beginning of the zip file.
Because the extra field has a maximum length of
$2^{16}-1$~bytes,
it is only possible to quote up to \num{1808}
local file headers
(or \num{1170} with Zip64),
assuming that filenames are allocated as in \autoref{sec:filenames}.
(In the case of DEFLATE,
you can use extra-field quoting for the first (shortest) local file headers,
then switch to DEFLATE quoting for the remainder.)
Another problem is that, in order to conform
to the internal data structure of the extra field,
you must select a 16-bit type tag~\cite[\S 4.5.2]{appnote}
to precede the quoted data.
We want to choose a type tag that will cause parsers
to ignore the quoted data, rather than trying to interpret it
as meaningful metadata.
Zip parsers are supposed to ignore unknown type tags,
so we could choose a type tag at random,
but there is the risk that the tag may be allocated in the future,
breaking the compatibility of the construction.

\autoref{fig:zipped-size}
illustrates the possibility of using extra-field quoting
with bzip2, with and without Zip64.
Both lines have a knee at which
the growth transitions from quadratic to linear.
In the non-Zip64 case, the knee occurs where the maximum uncompressed file size
($2^{32}-2$~bytes) is reached;
after this point, one can only increase the number of files, not their size.
The line ends completely when the number of files reaches \num{1809},
when we run out of room in the
extra field to quote quote additional headers.
In the Zip64 case, the knee occurs at \num{1171} files,
after which the size of files can be increased but not their number.
Extra-field quoting also helps in the case of DEFLATE,
but the difference is so slight as to be visually imperceptible
and so it has been omitted from the figure.
It increases the compression ratio of
% zbsm.zip        5461307620 / 42374      128883.45730872705
% zbsm.extra.zip  5525059932 / 42374      130387.97215273516
% >>> 130387.97215273516 / 128883.45730872705 - 1.0
% 0.01167345193420899
zbsm.zip by 1.2\%;
% zblg.zip        281395456244934 / 9893525       28442385.9286689
% zblg.extra.zip  281399207414622 / 9891773       28447802.77657221
% >>> 28447802.77657221 / 28442385.9286689 - 1.0
% 0.00019044984189764413
zblg.zip by 0.019\%;
% zbxl.zip        4507981427706459 / 45876952     98262444.01996146
% zbxl.extra.zip  4507981392085269 / 45875782     98264949.29471217
% >>> "%.20f" % (98264949.29471217 / 98262444.01996146 - 1.0)
% '0.00002549575044352714'
and zbxl.zip by 0.0025\%.


\section{Discussion}
\label{sec:discussion}

In related work,
Plötz et~al.~\cite[\S 4]{SAR-PR-2006-04}
used overlapping files to create a
near-self-replicating zip file.
Gynvael Coldwind~\cite[p47]{gynvael}
has previously suggested
overlapping files in the style of \autoref{sec:overlap}.

We have designed the quoted-overlap zip bomb construction for compatibility,
taking into consideration a number of implementation differences,
some of which are shown in \autoref{tab:compatibility}.
The resulting construction is compatible with zip parsers that work
in the usual back-to-front way,
first consulting the central directory
and using it as an index of files.
Among these is the example
zip parser included in Nail~\cite{186219},
which is automatically generated from a formal grammar.
The construction is not compatible, however,
with ``streaming'' parsers,
those that parse the zip file from beginning to end in one pass
without first reading the central directory.
By their nature, streaming parsers
do not permit any kind of file overlapping.
The most likely outcome is that they
will extract only the first file.
They may even raise an error besides,
as is the case with \mbox{sunzip}~\cite{sunzip},
which parses the central directory at the end and checks it for consistency
with the local file headers it has already seen.

If you need the extracted files to start with a certain prefix
other than the bytes of a local file header,
you can insert a DEFLATE block before the
non-compressed block that quotes the next header.
Not every file in the zip file has to participate in the bomb construction:
you can also include ordinary files
if needed to conform to some special format.
(The source code has a \texttt{-{}-{}template}
option to facilitate this use case.)
Many file formats use zip as a container;
examples are Java JAR, Android APK, and LibreOffice documents.

Detecting the specific class of zip bomb we have developed in this article is easy:
just look for overlapping files.
Mark Adler has written \href{https://github.com/madler/unzip/commits/6519bf0f8a896851d9708da11e1b63c818238c8f}{a patch}
for Info-ZIP UnZip that does just that.
In general, though, rejecting overlapping files
does not protect against all classes of zip bomb.
It is difficult to predict in advance
whether a zip file is a bomb or not,
unless you have precise knowledge of the internals of the parsers
that will be used to parse it.
Peeking into the headers and summing
the ``uncompressed size'' fields of all files
does not work, in general,
because the value stored in the headers
may not agree with the actual uncompressed size.
(See the ``permits too-short file size'' row in \autoref{tab:compatibility}.)
Robust protection against zip bombs
involves placing time, memory, and disk space limits
on the zip parser while it operates.
Handle zip parsing,
as any complex operation on untrusted data,
with caution.


\section*{Acknowledgements}

I~thank
Mark Adler,
Russ Cox,
Brandon Enright,
Marek Majkowski,
Josh Wolfe,
and the WOOT reviewers
for comments on a draft of this article.
Caolán McNamara evaluated the security impact
on LibreOffice.


\section*{Availability}

The zip bombs developed in this article,
the programs used to create them,
and the source code for the article itself
as well as the diagrams
are available from
\url{https://www.bamsoftware.com/hacks/zipbomb/}.
The artifacts prepared for WOOT are at
\url{https://www.bamsoftware.com/hacks/zipbomb/zipbomb-woot19.zip}.

\bibliographystyle{plain}
\bibliography{zipbomb}

\end{document}
